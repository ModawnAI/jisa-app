##### ê¸°ë³¸ ì •ë³´ ì„¤ì • ë‹¨ê³„
# import
from fastapi import Request, FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import openai
from openai import OpenAI
from google import genai
from google.genai import types
import threading   # ë™ì‹œì— ì—¬ëŸ¬ ì‘ì—…(ê·¸ë¦¼ ìƒì„±, ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ ì „ì†¡)ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” íŒ¨í‚¤ì§€
import time
import queue as q   # ì¹´ì¹´ì˜¤í†¡ ë‹µë³€ì„ íì— ì €ì¥í•¨
import os    # ë‹µë³€ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•  ë•Œ ì €ì¥ ê²½ë¡œ ìƒì„±í•˜ëŠ”ë° ì‚¬ìš©.
import requests
import asyncio
import aiohttp
import json
import tempfile
import shutil
from dotenv import load_dotenv
from pinecone_helper import query_pinecone, format_pinecone_results_for_gpt
from rag_chatbot import rag_answer
from commission_detector import detect_commission_query
from commission_service import query_commission, format_commission_for_gpt

# Load environment variables
load_dotenv()

# API Keys
API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

##### ê¸°ëŠ¥í•¨ìˆ˜ êµ¬í˜„ ë‹¨ê³„ #####

# ë©”ì‹œì§€ ì „ì†¡
# ChatGPTì˜ ë‹µë³€ì„ ì¹´ì¹´ì˜¤í†¡ ì„œë²„ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜.
def textReponseFormat(bot_response):
    response = {"version": "2.0", "template": {"outputs": [{"simpleText": {"text": bot_response}}], "quickReplies":[]}}

    return response


# ì‘ë‹µ ì´ˆê³¼ ì‹œ ë‹µë³€
# ë‹µë³€ ì‹œê°„ì´ ì§€ì—°ë˜ë©´ ì§€ì—° ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³ , ë‹µë³€ì„ ìš”ì²­í•˜ê¸° ìœ„í•œ ë²„íŠ¼ ìƒì„± í•¨ìˆ˜
def timeover():
    response = {"version": "2.0", "template":{
        "outputs":[
            {
                "simpleText": {
                    "text":"ì•„ì§ ì œê°€ ìƒê°ì´ ëë‚˜ì§€ ì•Šì•˜ì–´ìš”.ğŸ™â€â™‚ï¸ğŸ™â€â™‚ï¸ \nì ì‹œ í›„ ì•„ë˜ ë§í’ì„ ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”ğŸ‘†"
                }
            }
        ],
        "quickReplies":[
            {
                "action":"message",                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                "label":"ìƒê° ë‹¤ ëë‚¬ë‚˜ìš”?ğŸ™‹â€â™‚ï¸",  # ë²„íŠ¼ì— ì¶œë ¥í•  í…ìŠ¤íŠ¸íŠ¸
                "messageText": "ìƒê° ë‹¤ ëë‚¬ë‚˜ìš”?"  # ë²„íŠ¼ì„ í´ë¦­í–ˆì„ ë•Œ ì±„íŒ…ì°½ì— ìƒì„±ë˜ëŠ” ë©”ì‹œì§€
            }
        ]
    }}

    return response

# ChatGPTì—ê²Œ ì§ˆë¬¸/ë‹µë³€ ë°›ê¸° - NEW VERSION WITH COMMISSION DETECTION
def getTextFromGPT(prompt):
    """
    Uses commission detection first, then RAG chatbot
    - If commission query detected: routes to commission system
    - Otherwise: uses RAG chatbot with Gemini Flash + Pinecone + Gemini 2.5 Pro
    """
    # === STEP 1: Commission Detection ===
    print("=" * 80)
    print("ğŸ” Step 1: Commission Detection")
    detection_result = detect_commission_query(prompt)
    print(f"   Is Commission Query: {detection_result['is_commission_query']}")
    print(f"   Confidence: {detection_result['confidence']:.2f}")
    print(f"   Matched Keywords: {detection_result['matched_keywords']}")
    print(f"   Reasoning: {detection_result['reasoning']}")
    print("=" * 80)

    # === STEP 2: Route Based on Detection ===
    if detection_result['is_commission_query'] and detection_result['confidence'] >= 0.5:
        print("ğŸ¯ Routing to COMMISSION SYSTEM")
        print("=" * 80)
        try:
            # Query commission system
            commission_result = query_commission(prompt)

            # Format commission data as context for GPT (no emojis, plain text)
            commission_context = format_commission_for_gpt(commission_result)
            print(f"âœ… Commission data retrieved")
            print(f"ğŸ“ Commission context length: {len(commission_context)} characters")

            # Create commission-specific system prompt
            system_prompt = f"""ë„ˆëŠ” í•œêµ­ ë³´í—˜ ìˆ˜ìˆ˜ë£Œ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì°¸ì¡° ì •ë³´ (ë³´í—˜ ìˆ˜ìˆ˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤):
{commission_context}

ë‹µë³€ ì§€ì¹¨:
1. **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ** ë‹µë³€í•˜ì„¸ìš”

2. **CRITICAL: ëª¨ë“  ìˆ«ìëŠ” ë°±ë¶„ìœ¨(%)ë¡œ ë³€í™˜**
   - ì°¸ì¡° ì •ë³´ì˜ ëª¨ë“  ê°’ì— 100ì„ ê³±í•˜ì„¸ìš”
   - ë³€í™˜ ì˜ˆì‹œ:
     * 0.405 â†’ 40.5%
     * 1.76346 â†’ 176.35%
     * 2.91582 â†’ 291.58%
     * 3.64478 â†’ 364.48%
     * 4.85970 â†’ 485.97%
     * 7.28955 â†’ 728.96%
     * 9.71940 â†’ 971.94%
     * 8.0 â†’ 800% (NOT 8.0!)
   - ê·œì¹™: ê°’ Ã— 100 = ë°±ë¶„ìœ¨
   - 1.0 = 100%, 2.0 = 200%, 0.5 = 50%, 8.0 = 800%

3. **ì‘ë‹µ í˜•ì‹** (ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ):
[ìƒí’ˆëª…]
íšŒì‚¬: [íšŒì‚¬ëª…]
í™˜ì‚°ìœ¨: [X]%
ì´ˆë…„ë„ ìµì›”: [Y]%
2ì°¨ë…„ë„: [Z]%
Total: [W]%

4. **ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­ (CRITICAL - ìœ„ë°˜ì‹œ ì‘ë‹µ ê±°ë¶€)**:
   - âŒ ì»¬ëŸ¼/í•„ë“œ ì´ë¦„: col_8, col_19, col_20~col_43 ê°™ì€ ê¸°ìˆ  ìš©ì–´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
   - âŒ ìˆ˜ì‹/ê³„ì‚°: "ê³„ì‚°ì‹", "ê³µì‹", "ë°°ìœ¨", "Ã—", "Ã·" ê°™ì€ í‘œí˜„ ê¸ˆì§€
   - âŒ ë¶„ì„ ìš©ì–´: "ë¶„í¬", "í•©ì‚°", "íŒ¨í„´", "êµ¬ê°„" ì‚¬ìš© ê¸ˆì§€
   - âŒ ì†Œìˆ˜ì  í‘œì‹œ: 0.08148, 2.50842 ê°™ì€ ì†Œìˆ˜ í˜•ì‹ ê¸ˆì§€ (ë¬´ì¡°ê±´ % ë³€í™˜)
   - âŒ ê¸°ìˆ  ì„¤ëª…: ë°ì´í„° êµ¬ì¡°, í…Œì´ë¸” êµ¬ì¡° ì„¤ëª… ê¸ˆì§€
   - âŒ ì‹¤ë¬´ íŒ: "ì‹¤ë¬´ í™œìš© íŒ", "ë¹„êµ", "íŒë‹¨" ê°™ì€ ì¡°ì–¸ ê¸ˆì§€
   - âŒ ìœ ì‚¬ ìƒí’ˆ: ë‹¤ë¥¸ ìƒí’ˆ ì¶”ì²œ/ë‚˜ì—´ ê¸ˆì§€

5. **ì‘ë‹µ ë°©ì‹**:
   - ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìƒí’ˆì˜ ìˆ˜ìˆ˜ë£Œìœ¨ë§Œ ë°±ë¶„ìœ¨ë¡œ ê°„ë‹¨íˆ ì œì‹œ
   - ìˆëŠ” ë°ì´í„°ë§Œ í‘œì‹œ (ì—†ìœ¼ë©´ "í•´ë‹¹ ì •ë³´ ì—†ìŒ"ì´ë¼ê³ ë§Œ í‘œì‹œ)
   - ì¶”ê°€ ì„¤ëª…, í•´ì„, ë¶„ì„ ì¼ì²´ ê¸ˆì§€

6. ë‹µë³€ ëì— ì¶œì²˜ ì¶”ê°€: ğŸ“š ì¶œì²˜: ë³´í—˜ìˆ˜ìˆ˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤
"""

            # Call Gemini with commission context
            print("=" * 80)
            print("ğŸ¤– Gemini ìš”ì²­ ì‹œì‘ (Commission)")
            print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {prompt}")
            print(f"ğŸ§  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)} ë¬¸ì")

            # Use Gemini Flash Latest
            client = genai.Client(api_key=GEMINI_API_KEY)
            model = "gemini-flash-latest"

            # Combine system prompt and user query
            full_prompt = f"{system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"

            # Create contents with proper structure
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=full_prompt)]
                )
            ]

            # Configure with ultrathink (maximum thinking budget)
            config = types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(thinking_budget=10000),  # UltraThink mode
                image_config=types.ImageConfig(image_size="1K")
            )

            # Stream response
            content = ""
            for chunk in client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=config
            ):
                if chunk.text:
                    content += chunk.text

            print("âœ… Gemini ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (Commission)")
            print(f"âœ… Commission GPT response extracted: {len(content)} characters")

            if content and content.strip():
                return content
            else:
                print("âŒ Failed to extract GPT response")
                raise Exception("No content in GPT response")

        except Exception as e:
            print(f"âŒ Commission ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            print("âš ï¸ Fallback to RAG...")
            # Fall through to RAG system

    # === STEP 3: Use RAG System (Default) ===
    print("ğŸ“š Routing to RAG SYSTEM")
    print("=" * 80)
    try:
        # Use the new RAG chatbot with top 10 retrieval
        answer = rag_answer(prompt, top_k=10)
        return answer
    except Exception as e:
        print(f"âŒ RAG ì±—ë´‡ ì˜¤ë¥˜: {e}")
        print("âš ï¸ Fallback to old Pinecone method...")

        # Fallback to old method if RAG fails
        pinecone_results = query_pinecone(prompt, top_k=10, rerank_top_n=5)
    
    if pinecone_results:
        # Pinecone ê²°ê³¼ë¥¼ GPTìš© ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        reference_content = format_pinecone_results_for_gpt(pinecone_results)
        print(f"âœ… Pineconeì—ì„œ {len(pinecone_results)}ê°œì˜ ê´€ë ¨ ì •ë³´ ì°¾ìŒ")
        
        # Extract only one source and one URL from the top Pinecone result
        source = ""
        source_url = ""
        if pinecone_results:
            source = pinecone_results[0].get('source', '')
            source_url = pinecone_results[0].get('source_url', '')
        
        # Format sources with URLs
        sources_text_parts = []
        if source:
            sources_text_parts.append(f"ğŸ“š ì¶œì²˜: {source}")
        if source_url:
            sources_text_parts.append(f"ğŸ”— ì°¸ì¡°ë§í¬: {source_url}")
        
        sources_text = "\n".join(sources_text_parts) if sources_text_parts else ""
    else:
        # Pinecone ê²°ê³¼ê°€ ì—†ì–´ë„ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€
        reference_content = "ì°¸ì¡° ë°ì´í„°ë² ì´ìŠ¤ì— ì§ì ‘ì ì¸ ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë§ˆì¼€íŒ… ì „ë¬¸ê°€ë¡œì„œì˜ ì¼ë°˜ì ì¸ ì§€ì‹ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        sources_text = ""
        print("âš ï¸ Pineconeì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•¨ - ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ì‹œë„")
    
    system_prompt = f"""ë„ˆëŠ” í•œêµ­ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    
    ì°¸ì¡° ì •ë³´ (ì‹¤ì œ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ë“¤ì˜ Q&A):
    {reference_content}
    
    ë‹µë³€ ì§€ì¹¨:
    1. **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ** ë‹µë³€í•˜ì„¸ìš”
    2. **300-500ìë¡œ ìƒì„¸í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš” (ì§§ì€ ë‹µë³€ ê¸ˆì§€!)
    3. **ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¹Šì´ ì´í•´í•˜ê³  í™•ì¥**í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
    4. ì°¸ì¡° ì •ë³´ê°€ ìˆë‹¤ë©´ ì´ë¥¼ í™œìš©í•˜ë˜, **ë‹¹ì‹ ì˜ ì „ë¬¸ ì§€ì‹ê³¼ ê²°í•©**í•˜ì—¬ ë” í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
    5. ì°¸ì¡° ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ê±°ë‚˜ ì—†ì–´ë„, **ë§ˆì¼€íŒ… ì „ë¬¸ê°€ë¡œì„œì˜ ì¼ë°˜ì ì¸ ì§€ì‹ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
    6. êµ¬ì²´ì ì¸ ì˜ˆì‹œ, ë‹¨ê³„ë³„ ì„¤ëª…, ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ë‹µë³€ì„ í’ë¶€í•˜ê²Œ í•˜ì„¸ìš”
    7. ì—¬ëŸ¬ ì°¸ì¡°ê°€ ìˆë‹¤ë©´ í•µì‹¬ì„ ì¢…í•©í•˜ê³ , ì¶”ê°€ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ë”í•˜ì„¸ìš”
    8. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
    9. ë‹µë³€ ëì— ì¶œì²˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”: {sources_text}
    
    ë‹µë³€ ì ‘ê·¼ë²•:
    - ì§ˆë¬¸ì˜ ë³¸ì§ˆì ì¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”
    - ì°¸ì¡° ì •ë³´ + ì¼ë°˜ ë§ˆì¼€íŒ… ì§€ì‹ì„ í†µí•©í•˜ì„¸ìš”
    - ì‹¤ìš©ì ì´ê³  í–‰ë™ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
    - í•„ìš”ì‹œ ê´€ë ¨ëœ íŠ¸ë Œë“œë‚˜ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ì„ ì–¸ê¸‰í•˜ì„¸ìš”
    
    ë‹µë³€ í˜•ì‹:
    [200ì ì´ë‚´ì˜ ëª…í™•í•˜ê³  í™•ì¥ëœ ì „ë¬¸ê°€ ë‹µë³€]
    
    {sources_text}"""

    client = OpenAI(api_key=API_KEY, timeout=60.0)  # Increased timeout for chat
    
    try:
        # Enhanced OpenAI Responses API with reasoning and better logging
        print("=" * 80)
        print("ğŸ¤– GPT ìš”ì²­ ì‹œì‘")
        print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {prompt}")
        print(f"ğŸ§  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)} ë¬¸ì")
        print(f"â° ìš”ì²­ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Prepare input with system and user messages
        input_messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user", 
                "content": prompt
            }
        ]
        
        print(f"ğŸ“¨ ì…ë ¥ ë©”ì‹œì§€ ìˆ˜: {len(input_messages)}")
        
        response = client.responses.create(
            model="gpt-5-nano",
            input=input_messages,
            text={
                "format": {
                    "type": "text"
                },
                "verbosity": "medium"
            },
            reasoning={
                "effort": "medium"
            },
            tools=[],
            store=True
        )
        
        print("âœ… GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        print(f"ğŸ” ì‘ë‹µ íƒ€ì…: {type(response)}")
        print(f"ğŸ“Š ì‘ë‹µ ì†ì„±: {dir(response)}")
        
        # Enhanced response debugging
        if hasattr(response, '__dict__'):
            print(f"ğŸ“‹ ì‘ë‹µ ì „ì²´ êµ¬ì¡°: {response.__dict__}")
        
        # Try multiple extraction methods
        content = None
        extraction_method = "unknown"
        
        # Method 1: Check for output attribute
        if hasattr(response, 'output') and response.output:
            print("ğŸ”„ ì¶”ì¶œ ë°©ë²• 1: output ì†ì„± ì‚¬ìš©")
            if isinstance(response.output, list) and len(response.output) > 0:
                # Look for message type in output array
                print(f"   ğŸ“Š ì´ output í•­ëª© ìˆ˜: {len(response.output)}")
                for i, output_item in enumerate(response.output):
                    item_type = type(output_item).__name__
                    item_obj_type = getattr(output_item, 'type', 'no_type_attr')
                    print(f"   ğŸ“‹ output[{i}] í´ë˜ìŠ¤: {item_type}, type ì†ì„±: {item_obj_type}")
                    
                    # Show all attributes for debugging
                    if hasattr(output_item, '__dict__'):
                        attrs = list(output_item.__dict__.keys())[:5]  # First 5 attributes
                        print(f"   ğŸ” output[{i}] ì£¼ìš” ì†ì„±: {attrs}")
                    
                    if hasattr(output_item, 'type') and output_item.type == 'message':
                        print(f"   âœ… ë©”ì‹œì§€ íƒ€ì… ë°œê²¬: output[{i}]")
                        if hasattr(output_item, 'content') and output_item.content:
                            if isinstance(output_item.content, list) and len(output_item.content) > 0:
                                text_item = output_item.content[0]
                                if hasattr(text_item, 'text'):
                                    content = text_item.text
                                    extraction_method = f"output[{i}].content[0].text"
                                    break
                                else:
                                    print(f"   âš ï¸ text_item ì†ì„±: {dir(text_item)}")
                    elif hasattr(output_item, 'text') and output_item.text:
                        content = output_item.text
                        extraction_method = f"output[{i}].text"
                        print(f"   âœ… í…ìŠ¤íŠ¸ ë°œê²¬: output[{i}].text")
                        break
                    elif hasattr(output_item, 'content') and output_item.content:
                        # Try to extract from content if it's a string
                        if isinstance(output_item.content, str):
                            content = output_item.content
                            extraction_method = f"output[{i}].content"
                            print(f"   âœ… ì½˜í…ì¸  ë°œê²¬: output[{i}].content")
                            break
                        # Try to extract from content if it's a list with text items
                        elif isinstance(output_item.content, list) and len(output_item.content) > 0:
                            for j, content_item in enumerate(output_item.content):
                                if hasattr(content_item, 'text') and content_item.text:
                                    content = content_item.text
                                    extraction_method = f"output[{i}].content[{j}].text"
                                    print(f"   âœ… ì¤‘ì²© í…ìŠ¤íŠ¸ ë°œê²¬: output[{i}].content[{j}].text")
                                    break
                            if content:
                                break
                
                # Fallback to first item if no message found
                if not content:
                    output_item = response.output[0]
                    content = str(output_item)
                    extraction_method = "output[0] string conversion"
            else:
                content = str(response.output)
                extraction_method = "output string conversion"
        
        # Method 2: Check for text attribute
        elif hasattr(response, 'text'):
            print("ğŸ”„ ì¶”ì¶œ ë°©ë²• 2: text ì†ì„± ì‚¬ìš©")
            content = response.text
            extraction_method = "direct text attribute"
        
        # Method 3: Check for choices (like chat completions)
        elif hasattr(response, 'choices') and response.choices:
            print("ğŸ”„ ì¶”ì¶œ ë°©ë²• 3: choices ì†ì„± ì‚¬ìš©")
            if len(response.choices) > 0 and hasattr(response.choices[0], 'message'):
                content = response.choices[0].message.content
                extraction_method = "choices[0].message.content"
        
        # Method 4: String conversion fallback
        else:
            print("ğŸ”„ ì¶”ì¶œ ë°©ë²• 4: ì „ì²´ ì‘ë‹µ ë¬¸ìì—´ ë³€í™˜")
            content = str(response)
            extraction_method = "full response string conversion"
        
        print(f"ğŸ¯ ì‚¬ìš©ëœ ì¶”ì¶œ ë°©ë²•: {extraction_method}")
        print(f"ğŸ“ ì¶”ì¶œëœ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: '{content[:100] if content else 'None'}...'")
        print(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(content) if content else 0} ë¬¸ì")
        
        # Check for reasoning if available
        if hasattr(response, 'reasoning'):
            print(f"ğŸ§  ì¶”ë¡  ì •ë³´ í¬í•¨ë¨: {response.reasoning}")
        
        print("=" * 80)
        
        if content and content.strip():
            return content
        else:
            print("âŒ ë¹ˆ ì‘ë‹µ ê°ì§€")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. GPTì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
    except Exception as e:
        print("=" * 80)
        print(f"âŒ GPT ì‘ë‹µ ì˜¤ë¥˜ ë°œìƒ")
        print(f"ğŸš¨ ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        print(f"ğŸ“„ ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
        print(f"â° ì˜¤ë¥˜ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# í…ìŠ¤íŠ¸ íŒŒì¼ ì´ˆê¸°í™”
# ë©”ì¸ í•¨ìˆ˜ì—ì„œ 3.5ì´ˆ ì´í›„ì— ìƒì„±ëœ ë‹µë³€ ë˜ëŠ” ê·¸ë¦¼ ì •ë³´ëŠ” ì„ì‹œë¡œ ì €ì¥í•¨.
# ì‚¬ìš©ìì—ê²Œ ë‹µë³€ì„ ì „ì†¡í•œ í›„ì—ëŠ” í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ ì´ë¯¸ì‹œ ì €ì¥ ì •ë³´ ì´ˆê¸°í™”í•¨.
def dbReset(filename):
    with open(filename, 'w') as f:
        f.write("")

# Callback ì²˜ë¦¬ í•¨ìˆ˜
async def send_callback_response(callback_url, response_text):
    """
    ì½œë°± URLë¡œ ì‘ë‹µì„ ì „ì†¡í•©ë‹ˆë‹¤.
    
    Args:
        callback_url (str): ì¹´ì¹´ì˜¤ì—ì„œ ì œê³µí•œ ì½œë°± URL
        response_text (str): ì „ì†¡í•  ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        print(f"ì½œë°± ì‘ë‹µ ì „ì†¡ ì‹œì‘: {callback_url}")
        
        # ì¹´ì¹´ì˜¤ ì±—ë´‡ ì½œë°± ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ì„¤ì •
        response_data = {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": response_text
                        }
                    }
                ]
            }
        }
        
        headers = {
            'Content-Type': 'application/json',
        }
        print("ì‘ë‹µ ë°ì´í„°:", json.dumps(response_data, ensure_ascii=False)[:200] + "...")
        
        async with aiohttp.ClientSession() as session:
            async with session.post(callback_url, json=response_data, headers=headers) as resp:
                if resp.status == 200:
                    print(f"ì½œë°± ì‘ë‹µ ì „ì†¡ ì„±ê³µ: {callback_url}")
                    response_text = await resp.text()
                    print(f"ì½œë°± ì‘ë‹µ: {response_text}")
                else:
                    response_text = await resp.text()
                    print(f"ì½œë°± ì‘ë‹µ ì „ì†¡ ì‹¤íŒ¨: {resp.status}, {response_text}")
    except Exception as e:
        print(f"ì½œë°± ì‘ë‹µ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

async def process_callback_response(callback_url, question, delay_seconds=2):
    """
    GPTë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ì½œë°± URLë¡œ ì „ì†¡í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜
    
    Args:
        callback_url (str): ì¹´ì¹´ì˜¤ì—ì„œ ì œê³µí•œ ì½œë°± URL
        question (str): ì‚¬ìš©ìì˜ ì§ˆë¬¸
        delay_seconds (int): ì§€ì—° ì‹œê°„(ì´ˆ)
    """
    try:
        # ì‹¤ì œ ì²˜ë¦¬ ì „ ëŒ€ê¸° ì‹œê°„ (ì„ íƒì )
        await asyncio.sleep(delay_seconds)
        
        # GPTë¡œ ë‹µë³€ ìƒì„±
        print(f"ì§ˆë¬¸: {question}ì— ëŒ€í•œ GPT ì‘ë‹µ ìƒì„± ì¤‘...")
        answer = getTextFromGPT(question)
        print(f"ìƒì„±ëœ ë‹µë³€: {answer[:100]}...")
        
        # ë¹ˆ ë‹µë³€ì´ë©´ ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
        if not answer or answer.strip() == "":
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            print("ë¹ˆ ë‹µë³€ ê°ì§€, ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´")
        
        # ì½œë°± URLë¡œ ì‘ë‹µ ì „ì†¡
        await send_callback_response(callback_url, answer)
        print(f"GPT ì‘ë‹µ ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        error_message = f"GPT ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_message)
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ
        await send_callback_response(callback_url, f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def processCallback(callback_data):
    # Callbackì—ì„œ ë°›ì€ ë°ì´í„° ì²˜ë¦¬
    user_id = callback_data.get('userRequest', {}).get('user', {}).get('id', 'unknown')
    utterance = callback_data.get('userRequest', {}).get('utterance', '')
    
    print(f"Processing callback for user {user_id}: {utterance}")
    
    # ChatGPT ì‘ë‹µ ìƒì„±
    bot_response = getTextFromGPT(utterance)
    response = textReponseFormat(bot_response)
    
    return response

# PDF ì²˜ë¦¬ í•¨ìˆ˜
def process_pdf_with_openai(file_path: str) -> str:
    """PDF íŒŒì¼ì„ OpenAI responses APIë¡œ ì§ì ‘ ì²˜ë¦¬"""
    try:
        client = OpenAI(api_key=API_KEY, timeout=300.0)  # Increased to 5 minutes
        
        # 1. íŒŒì¼ì„ OpenAIì— ì—…ë¡œë“œ
        with open(file_path, 'rb') as file:
            uploaded_file = client.files.create(
                file=file,
                purpose='assistants'
            )
        
        # 2. ì—…ë¡œë“œëœ íŒŒì¼ IDë¥¼ ì‚¬ìš©í•˜ì—¬ responses API í˜¸ì¶œ
        response = client.responses.create(
            model="gpt-4.1",
            input=[
                {
                    "role": "user",
                    "content": [
                        { 
                            "type": "input_text", 
                            "text": "ì´ PDF íŒŒì¼ì˜ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì—„ì²­ ìì„¸í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ê³¼ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ í¬í•¨í•´ì£¼ì„¸ìš”." 
                        },
                        {
                            "type": "input_file",
                            "file_id": uploaded_file.id
                        }
                    ]
                }
            ]
        )
        
        # 3. ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ (ì •ë¦¬)
        try:
            client.files.delete(uploaded_file.id)
        except:
            pass  # ì‚­ì œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        # 4. ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if hasattr(response, 'output') and response.output:
            if isinstance(response.output, list) and len(response.output) > 0:
                output_item = response.output[0]
                if hasattr(output_item, 'text'):
                    return output_item.text
                elif hasattr(output_item, 'content'):
                    return output_item.content
                else:
                    return str(output_item)
            else:
                return str(response.output)
        elif hasattr(response, 'text'):
            return response.text
        else:
            return str(response)
            
    except Exception as e:
        error_msg = str(e)
        print(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {error_msg}")
        
        # íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë©”ì‹œì§€
        if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
            return f"PDF ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        else:
            return f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"

def append_to_reference_file(content: str, pdf_filename: str, filename: str = "reference.txt"):
    """ì¶”ì¶œëœ ë‚´ìš©ì„ reference.txt íŒŒì¼ì— ì¶”ê°€ (ê¹”ë”í•œ í…ìŠ¤íŠ¸ë§Œ)"""
    try:
        file_path = os.path.join(os.path.dirname(__file__), filename)
        
        # í˜„ì¬ ì‹œê°„ì„ í•œêµ­ì–´ í˜•ì‹ìœ¼ë¡œ ìƒì„±
        import datetime
        now = datetime.datetime.now()
        
        # ìš”ì¼ í•œêµ­ì–´ ë³€í™˜
        weekdays = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
        weekday_kr = weekdays[now.weekday()]
        
        # ì˜¤ì „/ì˜¤í›„ êµ¬ë¶„
        if now.hour < 12:
            ampm = "ì˜¤ì „"
            hour_12 = now.hour if now.hour != 0 else 12
        else:
            ampm = "ì˜¤í›„"
            hour_12 = now.hour if now.hour <= 12 else now.hour - 12
        
        # í•œêµ­ì–´ ì‹œê°„ í˜•ì‹ ìƒì„±
        korean_time = f"{now.year}ë…„ {now.month}ì›” {now.day}ì¼ ({weekday_kr}) {ampm} {hour_12}ì‹œ {now.minute}ë¶„"
        
        # ê¸°ì¡´ ë‚´ìš© ì½ê¸°
        existing_content = ""
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                existing_content = f.read()
                
        # ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        if "ìµœì¢… ì—…ë°ì´íŠ¸:" in existing_content:
            # ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ë¼ì¸ì„ ìƒˆë¡œìš´ ì‹œê°„ìœ¼ë¡œ êµì²´
            lines = existing_content.split('\n')
            for i, line in enumerate(lines):
                if line.startswith("ìµœì¢… ì—…ë°ì´íŠ¸:"):
                    lines[i] = f"ìµœì¢… ì—…ë°ì´íŠ¸: {korean_time}"
                    break
            existing_content = '\n'.join(lines)
        else:
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ ë§¨ ì•ì— ì¶”ê°€
            timestamp_header = f"ìµœì¢… ì—…ë°ì´íŠ¸: {korean_time}\nì°¸ì¡° íŒŒì¼: reference.txt\n\n"
            existing_content = timestamp_header + existing_content
        
        # ResponseOutputText í˜•íƒœì˜ ë‚´ìš©ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        clean_text = content
        
        # ResponseOutputText íŒ¨í„´ì´ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        print(f"ğŸ” ì›ë³¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")
        
        if "[ResponseOutputText(" in content and "text='" in content:
            try:
                import re
                
                # ResponseOutputText ì „ì²´ êµ¬ì¡°ì—ì„œ text ë¶€ë¶„ë§Œ ì¶”ì¶œ
                # [ResponseOutputText(annotations=[], text='ì‹¤ì œë‚´ìš©', type='output_text', logprobs=[])]
                pattern = r"text='(.*?)'(?:,\s*type='output_text')"
                match = re.search(pattern, content, re.DOTALL)
                
                if match:
                    clean_text = match.group(1)
                    # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìë“¤ ë³µì›
                    clean_text = clean_text.replace("\\'", "'")
                    clean_text = clean_text.replace('\\"', '"')
                    clean_text = clean_text.replace('\\n', '\n')
                    clean_text = clean_text.replace('\\t', '\t')
                    clean_text = clean_text.replace('\\r', '\r')
                    print(f"âœ… ì •ê·œì‹ìœ¼ë¡œ ê¹”ë”í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ê¸¸ì´: {len(clean_text)})")
                    print(f"ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {clean_text[:100]}...")
                else:
                    print("âš ï¸ ì •ê·œì‹ ë§¤ì¹­ ì‹¤íŒ¨, ìˆ˜ë™ ì¶”ì¶œ ì‹œë„")
                    # ìˆ˜ë™ ì¶”ì¶œ ì‹œë„
                    start_marker = "text='"
                    start_idx = content.find(start_marker)
                    if start_idx != -1:
                        start_idx += len(start_marker)
                        
                        # ë íŒ¨í„´ ì°¾ê¸°
                        end_marker = "', type='output_text'"
                        end_idx = content.find(end_marker, start_idx)
                        
                        if end_idx != -1:
                            clean_text = content[start_idx:end_idx]
                            # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìë“¤ ë³µì›
                            clean_text = clean_text.replace("\\'", "'")
                            clean_text = clean_text.replace('\\"', '"')
                            clean_text = clean_text.replace('\\n', '\n')
                            clean_text = clean_text.replace('\\t', '\t')
                            clean_text = clean_text.replace('\\r', '\r')
                            print(f"âœ… ìˆ˜ë™ ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(clean_text)})")
                            print(f"ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {clean_text[:100]}...")
                        else:
                            print("âŒ ìˆ˜ë™ ì¶”ì¶œë„ ì‹¤íŒ¨")
                    else:
                        print("âŒ text=' ë§ˆì»¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
            except Exception as e:
                print(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                
        elif "text='" in content:
            print("ğŸ” ë‹¨ìˆœ text=' íŒ¨í„´ ê°ì§€, ì¶”ì¶œ ì‹œë„")
            try:
                start_marker = "text='"
                start_idx = content.find(start_marker)
                if start_idx != -1:
                    start_idx += len(start_marker)
                    
                    # ë‹¤ì–‘í•œ ë íŒ¨í„´ ì‹œë„
                    end_patterns = ["', type='output_text'", "', type=", "', logprobs=", "'$"]
                    end_idx = -1
                    
                    for pattern in end_patterns:
                        temp_idx = content.find(pattern, start_idx)
                        if temp_idx != -1:
                            end_idx = temp_idx
                            print(f"âœ… ë íŒ¨í„´ ë°œê²¬: {pattern}")
                            break
                    
                    if end_idx != -1:
                        clean_text = content[start_idx:end_idx]
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìë“¤ ë³µì›
                        clean_text = clean_text.replace("\\'", "'")
                        clean_text = clean_text.replace('\\"', '"')
                        clean_text = clean_text.replace('\\n', '\n')
                        clean_text = clean_text.replace('\\t', '\t')
                        clean_text = clean_text.replace('\\r', '\r')
                        print(f"âœ… ë‹¨ìˆœ íŒ¨í„´ ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(clean_text)})")
                        print(f"ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {clean_text[:100]}...")
            except Exception as e:
                print(f"âš ï¸ ë‹¨ìˆœ íŒ¨í„´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        else:
            print("â„¹ï¸ ResponseOutputText íŒ¨í„´ì´ ì—†ìŒ, ì›ë³¸ ì‚¬ìš©")
            
        print(f"ğŸ¯ ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(clean_text)}")
        
        # ìƒˆ ë‚´ìš© ì¶”ê°€ (íŒŒì¼ëª…ê³¼ ê¹”ë”í•œ í…ìŠ¤íŠ¸ë§Œ)
        new_content = f"\n\nFILENAME: {pdf_filename}\n{clean_text}\n--- ë‚´ìš© ë ---\n"
        
        # íŒŒì¼ì— ì“°ê¸°
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(existing_content + new_content)
        
        print(f"PDF ë‚´ìš©ì´ {filename}ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤ (íŒŒì¼ëª…: {pdf_filename})")
        return True
        
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
        return False

##### (3) ì„œë²„ ìƒì„± ë‹¨ê³„
app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # Next.js ê°œë°œ ì„œë²„
        "https://ann-inventional-marisela.ngrok-free.app",  # ngrok ë„ë©”ì¸
        "https://*.ngrok-free.app",  # ëª¨ë“  ngrok ë„ë©”ì¸ í—ˆìš©
        "https://jisa.flowos.work",  # Production domain
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "kakaoTest"}

@app.post("/")    # POST to root when using --root-path /chat (becomes /chat/ externally)
async def chat_root(request: Request):
    kakaorequest = await request.json()
    return await mainChat(kakaorequest)   # ì„œë²„ë¡œ ë‹µë³€ ì „ì†¡

@app.post("/callback/")    # Callback URL for delayed responses
async def callback(request: Request):
    callback_data = await request.json()
    return processCallback(callback_data)

@app.post("/upload-pdf")
async def upload_pdf(file: UploadFile = File(...)):
    """PDF íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸ (ì¦‰ì‹œ ì‘ë‹µ)"""
    try:
        # íŒŒì¼ íƒ€ì… ê²€ì¦
        if file.content_type != "application/pdf":
            raise HTTPException(status_code=400, detail="Only PDF files are allowed")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            shutil.copyfileobj(file.file, tmp_file)
            tmp_file_path = tmp_file.name
        
        # ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬)
        asyncio.create_task(process_pdf_background(tmp_file_path, file.filename))
        
        return {
            "message": "PDF upload successful, processing in background",
            "filename": file.filename,
            "status": "processing"
        }
            
    except Exception as e:
        print(f"PDF ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"PDF upload failed: {str(e)}")

async def process_pdf_background(tmp_file_path: str, filename: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ PDF ì²˜ë¦¬"""
    try:
        print(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ PDF ì²˜ë¦¬ ì‹œì‘: {filename}")
        
        # PDF ì²˜ë¦¬
        extracted_content = process_pdf_with_openai(tmp_file_path)
        
        # reference.txtì— ì¶”ê°€ (íŒŒì¼ëª…ê³¼ í•¨ê»˜)
        success = append_to_reference_file(extracted_content, filename)
        
        if success:
            print(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {filename}")
        else:
            print(f"âŒ PDF ì €ì¥ ì‹¤íŒ¨: {filename}")
            
    except Exception as e:
        print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.unlink(tmp_file_path)
            print(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {tmp_file_path}")
        except:
            pass

##### (4) ë©”ì¸ í•¨ìˆ˜ êµ¬í˜„ ë‹¨ê³„ #####

# ë©”ì¸ í•¨ìˆ˜
async def mainChat(kakaorequest):
    # ì‚¬ìš©ì ë°œí™” í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    utterance = kakaorequest.get("userRequest", {}).get("utterance", "")
    # ì¹´ì¹´ì˜¤ callback URL ê°€ì ¸ì˜¤ê¸°
    callback_url = kakaorequest.get("userRequest", {}).get("callbackUrl", "")
    
    print(f'ì‚¬ìš©ì ì§ˆë¬¸: {utterance}')
    print(f'ì½œë°± URL: {callback_url}')
    
    # ì½œë°± URLì´ ìˆëŠ” ê²½ìš° ë¹„ë™ê¸° ì²˜ë¦¬
    if callback_url:
        # ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜ (useCallback í•„ìˆ˜)
        temp_response = {
            "version": "2.0",
            "useCallback": True,
            "data": {
                "text": "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."
            }
        }
        
        # ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        try:
            task = asyncio.create_task(process_callback_response(callback_url, utterance))
            print(f"ë¹„ë™ê¸° GPT ì‘ë‹µ íƒœìŠ¤í¬ ìƒì„±ë¨")
        except Exception as e:
            print(f"íƒœìŠ¤í¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return temp_response
    else:
        # ì½œë°± URLì´ ì—†ëŠ” ê²½ìš° ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
        try:
            # ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ run_until_complete ì‚¬ìš©
            answer = getTextFromGPT(utterance)
            
            # ì¹´ì¹´ì˜¤ ì±—ë´‡ ì‘ë‹µ í˜•ì‹
            response_body = {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {
                            "simpleText": {
                                "text": answer
                            }
                        }
                    ]
                }
            }
            return response_body
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‘ë‹µ
            error_message = f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            response_body = {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {
                            "simpleText": {
                                "text": error_message
                            }
                        }
                    ]
                }
            }
            return response_body  

# ChatGPTë‹µë³€/ DALL.E ì‚¬ì§„ ìš”ì²­ ë° ì‘ë‹µ í™•ì¸ í•¨ìˆ˜
def responseOpenAI(request, response_queue, filename):
    # ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë‹µë³€ ì™„ì„± ì—¬ë¶€ë¥¼ ìš”ì²­í•œ ê²½ìš°
    if 'ìƒê° ë‹¤ ëë‚¬ë‚˜ìš”?' in request["userRequest"]["utterance"]:
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì—´ê¸°
        with open(filename) as f:
            last_update = f.read()

        # í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ ì €ì¥ëœ ì •ë³´ê°€ ìˆì„ ê²½ìš° íì— ì €ì¥
        if len(last_update.split()) > 1:
            kind, bot_res, prompt = last_update.split()[0], last_update.split()[1], last_update.split()[2]
            response_queue.put(textReponseFormat(bot_res))
            dbReset(filename)

    # ì¼ë°˜ ì±„íŒ…ì¸ ê²½ìš° (ëª¨ë“  ë©”ì‹œì§€ë¥¼ ChatGPTì—ê²Œ ì „ë‹¬)
    else:
        dbReset(filename)
        prompt = request["userRequest"]["utterance"]
        print(f"Processing prompt: {prompt}")
        bot_res = getTextFromGPT(prompt)
        print(f"GPT response: {bot_res}")
        response_queue.put(textReponseFormat(bot_res))
        save_log = "ask " + str(bot_res) + " " + str(prompt)
        with open(filename, 'w') as f:
            f.write(save_log)