#!/usr/bin/env python3
"""
Ultra-granular schedule upload to Pinecone.
Creates individual vectors for each event, time slot, and queryable attribute.
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any
from dotenv import load_dotenv
from pinecone import Pinecone
from openai import OpenAI
import time
from datetime import datetime, timedelta

load_dotenv()

# Initialize clients
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Constants
INDEX_NAME = "hof-branch-chatbot"
NAMESPACE = "hof-knowledge-base-max"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# File paths
script_dir = Path(__file__).parent
SCHEDULE_FILE = script_dir / "MODIFIED" / "Schedule.txt"
SCHEDULE_2_FILE = script_dir / "MODIFIED" / "Schedule_2.txt"


def get_embedding(text: str) -> List[float]:
    """Generate OpenAI embedding for text."""
    response = openai_client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text,
        dimensions=EMBEDDING_DIMENSIONS
    )
    return response.data[0].embedding


def parse_date_range(date_range: str) -> tuple:
    """Parse date range string into start and end dates."""
    if ".." in date_range:
        start, end = date_range.split("..")
        return start, end
    return date_range, date_range


def extract_individual_events_schedule1(data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extract ultra-granular vectors from Schedule.txt (daily events).
    Each event gets its own vector with full metadata.
    """
    chunks = []
    month = data.get("month", "2025-11")

    by_date = data.get("by_date", {})

    for date_str, events in by_date.items():
        # Parse date
        date_obj = datetime.strptime(date_str, "%Y-%m-%d")
        weekday = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"][date_obj.weekday()]

        for event_idx, event in enumerate(events):
            title = event.get("title", "")
            time_slot = event.get("time", "")
            location = event.get("location", "")
            presenter = event.get("presenter", "")
            category = event.get("category", "ì¼ë°˜ êµìœ¡")

            # Create searchable text
            searchable_text = f"""ë‚ ì§œ: {date_str} ({weekday})
ì œëª©: {title}"""

            if time_slot:
                searchable_text += f"\nì‹œê°„: {time_slot}"
            if location:
                searchable_text += f"\nìž¥ì†Œ: {location}"
            if presenter:
                searchable_text += f"\nê°•ì‚¬: {presenter}"
            searchable_text += f"\nì¹´í…Œê³ ë¦¬: {category}"

            # Natural language description
            natural_desc = f"{date_obj.month}ì›” {date_obj.day}ì¼ {weekday} "
            if time_slot:
                natural_desc += f"{time_slot} "
            natural_desc += f"{title}"
            if presenter:
                natural_desc += f" (ê°•ì‚¬: {presenter})"
            if location:
                natural_desc += f" at {location}"

            # Extract time components
            start_time = None
            end_time = None
            if time_slot and "-" in time_slot:
                times = time_slot.split("-")
                if len(times) == 2:
                    start_time = times[0].strip()
                    end_time = times[1].strip()

            # Determine event type
            is_training = "êµìœ¡" in title or "ê³¼ì •" in title or "ê°•ì˜" in category
            is_exam = "ì‹œí—˜" in title or "ì‘ì‹œ" in title
            is_orientation = "ì˜¤ë¦¬ì—”í…Œì´ì…˜" in title
            is_ceremony = "ìˆ˜ë£Œì‹" in title
            is_partner_education = category == "ì œíœ´ì‚¬ êµìœ¡"
            is_kblp = category == "KBLP ë³¸ì‚¬ ê°•ì˜" or "KBLP" in presenter
            is_zoom = "ZOOM" in title or "zoom" in title.lower()

            # Extract company/partner names
            companies = []
            if "ì‚¼ì„±í™”ìž¬" in title or "ì‚¼ì„±í™”ìž¬" in presenter:
                companies.append("ì‚¼ì„±í™”ìž¬")
            if "DBì†ë³´" in title or "DBì†ë³´" in presenter:
                companies.append("DBì†ë³´")
            if "KBë¼ì´í”„" in title or "KBLP" in presenter:
                companies.append("KBë¼ì´í”„")
            if "í•œí™”" in title:
                companies.append("í•œí™”ìƒëª…")

            # Create metadata
            metadata = {
                "chunk_type": "event_individual",
                "source_file": "Schedule.txt",
                "month": month,
                "date": date_str,
                "date_start": date_str,
                "date_end": date_str,
                "weekday": weekday,
                "day_of_month": date_obj.day,
                "title": title,
                "category": category,
                "searchable_text": searchable_text,
                "natural_description": natural_desc,

                # Boolean flags
                "is_training": is_training,
                "is_exam": is_exam,
                "is_orientation": is_orientation,
                "is_ceremony": is_ceremony,
                "is_partner_education": is_partner_education,
                "is_kblp": is_kblp,
                "is_zoom": is_zoom,
                "has_time": bool(time_slot),
                "has_location": bool(location),
                "has_presenter": bool(presenter),
            }

            # Add optional fields
            if time_slot:
                metadata["time"] = time_slot
                metadata["time_start"] = start_time
                metadata["time_end"] = end_time
            if location:
                metadata["location"] = location
            if presenter:
                metadata["presenter"] = presenter
            if companies:
                metadata["companies"] = companies

            chunks.append({
                "text": searchable_text,
                "metadata": metadata
            })

    return chunks


def extract_individual_events_schedule2(data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extract ultra-granular vectors from Schedule_2.txt (event ranges and weekly events).
    """
    chunks = []
    month = data.get("month", "2025-11")

    events = data.get("events", [])

    for event_idx, event in enumerate(events):
        title = event.get("title", "")

        # Handle date ranges
        date_range = event.get("date_range", "")
        week_window = event.get("week_window", "")

        if date_range:
            date_start, date_end = parse_date_range(date_range)
        elif week_window:
            date_start, date_end = parse_date_range(week_window)
        else:
            continue  # Skip events without dates

        # Parse dates
        start_obj = datetime.strptime(date_start, "%Y-%m-%d")
        end_obj = datetime.strptime(date_end, "%Y-%m-%d")
        duration_days = (end_obj - start_obj).days + 1

        # Get other fields
        time_slot = event.get("time", "")
        location = event.get("location", "")
        category = event.get("category", "")
        regions = event.get("regions", [])
        business_days = event.get("details", {}).get("business_days", 0)

        # Create searchable text
        searchable_text = f"""ì œëª©: {title}
ê¸°ê°„: {date_start} ~ {date_end} ({duration_days}ì¼ê°„)"""

        if time_slot:
            searchable_text += f"\nì‹œê°„: {time_slot}"
        if location:
            searchable_text += f"\nìž¥ì†Œ: {location}"
        if category:
            searchable_text += f"\nì¹´í…Œê³ ë¦¬: {category}"
        if regions:
            searchable_text += f"\nì§€ì—­: {', '.join(regions)}"
        if business_days:
            searchable_text += f"\nì˜ì—…ì¼: {business_days}ì¼"

        # Natural description
        natural_desc = f"{title} ({start_obj.month}ì›” {start_obj.day}ì¼"
        if duration_days > 1:
            natural_desc += f" ~ {end_obj.month}ì›” {end_obj.day}ì¼"
        natural_desc += ")"

        # Determine event type
        is_training = "êµìœ¡" in title or "ê³¼ì •" in title
        is_exam = "ì‹œí—˜" in title or category == "ì‹œí—˜"
        is_appointment = "ìœ„ì´‰" in title or "ì½”ë“œë°œê¸‰" in title
        is_deadline = "ë§ˆê°" in title or "ì ‘ìˆ˜" in title
        is_ceremony = "ìˆ˜ë£Œì‹" in title
        is_conference = "Conference" in title

        # Extract companies
        companies = []
        if "ì‚¼ì„±í™”ìž¬" in title:
            companies.append("ì‚¼ì„±í™”ìž¬")
        if "ì‚¼ì„±ìƒëª…" in title:
            companies.append("ì‚¼ì„±ìƒëª…")
        if "DBì†ë³´" in title or "DBì„¸ì¼ì¦ˆ" in title:
            companies.append("DBì†ë³´")
        if "KBë¼ì´í”„" in title or "KBLP" in title:
            companies.append("KBë¼ì´í”„")
        if "í•œí™”ìƒëª…" in title:
            companies.append("í•œí™”ìƒëª…")
        if "êµë³´ìƒëª…" in title:
            companies.append("êµë³´ìƒëª…")
        if "ë¯¸ëž˜ì—ì…‹" in title:
            companies.append("ë¯¸ëž˜ì—ì…‹")
        if "IMë¼ì´í”„" in title:
            companies.append("IMë¼ì´í”„")

        # Create metadata
        metadata = {
            "chunk_type": "event_range",
            "source_file": "Schedule_2.txt",
            "month": month,
            "date_start": date_start,
            "date_end": date_end,
            "duration_days": duration_days,
            "title": title,
            "searchable_text": searchable_text,
            "natural_description": natural_desc,

            # Boolean flags
            "is_training": is_training,
            "is_exam": is_exam,
            "is_appointment": is_appointment,
            "is_deadline": is_deadline,
            "is_ceremony": is_ceremony,
            "is_conference": is_conference,
            "has_time": bool(time_slot),
            "has_location": bool(location),
            "has_regions": bool(regions),
        }

        # Add optional fields
        if time_slot:
            metadata["time"] = time_slot
        if location:
            metadata["location"] = location
        if category:
            metadata["category"] = category
        if regions:
            metadata["regions"] = regions
        if companies:
            metadata["companies"] = companies
        if business_days:
            metadata["business_days"] = business_days

        chunks.append({
            "text": searchable_text,
            "metadata": metadata
        })

    return chunks


def extract_date_summaries(schedule1_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Create daily summary vectors for Schedule.txt.
    One vector per day summarizing all events on that day.
    """
    chunks = []
    month = schedule1_data.get("month", "2025-11")
    by_date = schedule1_data.get("by_date", {})

    for date_str, events in by_date.items():
        date_obj = datetime.strptime(date_str, "%Y-%m-%d")
        weekday = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"][date_obj.weekday()]

        # Create summary
        event_titles = [e.get("title", "") for e in events]
        event_count = len(events)

        searchable_text = f"""{date_str} ({weekday}) ì¼ì • ìš”ì•½
ì´ {event_count}ê°œ í–‰ì‚¬

í–‰ì‚¬ ëª©ë¡:
""" + "\n".join([f"{i+1}. {title}" for i, title in enumerate(event_titles)])

        natural_desc = f"{date_obj.month}ì›” {date_obj.day}ì¼ {weekday}ì—ëŠ” {event_count}ê°œ í–‰ì‚¬ ì˜ˆì •: {', '.join(event_titles[:3])}"
        if event_count > 3:
            natural_desc += f" ì™¸ {event_count - 3}ê°œ"

        metadata = {
            "chunk_type": "day_summary",
            "source_file": "Schedule.txt",
            "month": month,
            "date": date_str,
            "date_start": date_str,
            "date_end": date_str,
            "weekday": weekday,
            "day_of_month": date_obj.day,
            "event_count": event_count,
            "event_titles": event_titles,
            "searchable_text": searchable_text,
            "natural_description": natural_desc,
        }

        chunks.append({
            "text": searchable_text,
            "metadata": metadata
        })

    return chunks


def main():
    print("="*80)
    print("ULTRA-GRANULAR SCHEDULE UPLOAD TO PINECONE")
    print("="*80)

    # Load schedule files
    print(f"\nðŸ“– Loading schedule files...")
    with open(SCHEDULE_FILE, "r", encoding="utf-8") as f:
        schedule1_data = json.load(f)
    with open(SCHEDULE_2_FILE, "r", encoding="utf-8") as f:
        schedule2_data = json.load(f)

    print(f"âœ“ Loaded Schedule.txt: {len(schedule1_data.get('by_date', {}))} days")
    print(f"âœ“ Loaded Schedule_2.txt: {len(schedule2_data.get('events', []))} events")

    # Extract chunks
    print(f"\nðŸ”§ Extracting ultra-granular vectors...")

    chunks = []

    # Schedule 1: Individual events
    schedule1_events = extract_individual_events_schedule1(schedule1_data)
    chunks.extend(schedule1_events)
    print(f"  âœ“ Schedule.txt individual events: {len(schedule1_events)}")

    # Schedule 1: Daily summaries
    schedule1_summaries = extract_date_summaries(schedule1_data)
    chunks.extend(schedule1_summaries)
    print(f"  âœ“ Schedule.txt daily summaries: {len(schedule1_summaries)}")

    # Schedule 2: Event ranges
    schedule2_events = extract_individual_events_schedule2(schedule2_data)
    chunks.extend(schedule2_events)
    print(f"  âœ“ Schedule_2.txt event ranges: {len(schedule2_events)}")

    total_chunks = len(chunks)
    print(f"\nðŸ“Š Total vectors to create: {total_chunks}")

    # Generate embeddings and prepare vectors
    print(f"\nðŸ”® Generating embeddings...")
    vectors = []

    for i, chunk in enumerate(chunks, 1):
        if i % 10 == 0:
            print(f"  Progress: {i}/{total_chunks}")

        embedding = get_embedding(chunk["text"])

        vector_id = f"schedule_ultra_{i}_{time.time()}"

        vectors.append({
            "id": vector_id,
            "values": embedding,
            "metadata": chunk["metadata"]
        })

    print(f"âœ“ Generated {len(vectors)} embeddings")

    # Upload to Pinecone
    print(f"\nðŸ“¤ Uploading to Pinecone (namespace: {NAMESPACE})...")
    index = pc.Index(INDEX_NAME)

    # Upload in batches
    batch_size = 100
    for i in range(0, len(vectors), batch_size):
        batch = vectors[i:i+batch_size]
        index.upsert(vectors=batch, namespace=NAMESPACE)
        print(f"  âœ“ Uploaded batch {i//batch_size + 1}/{(len(vectors)-1)//batch_size + 1}")

    print(f"\nâœ… Upload complete!")

    # Get final stats
    time.sleep(2)
    stats = index.describe_index_stats()
    total_vectors = stats.namespaces.get(NAMESPACE, {}).vector_count or 0

    print(f"\n{'='*80}")
    print("FINAL STATISTICS")
    print(f"{'='*80}")
    print(f"Namespace: {NAMESPACE}")
    print(f"Total vectors in namespace: {total_vectors}")
    print(f"Newly added: {total_chunks}")
    print(f"\nBreakdown:")
    print(f"  â€¢ Individual events (Schedule.txt): {len(schedule1_events)}")
    print(f"  â€¢ Daily summaries (Schedule.txt): {len(schedule1_summaries)}")
    print(f"  â€¢ Event ranges (Schedule_2.txt): {len(schedule2_events)}")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
