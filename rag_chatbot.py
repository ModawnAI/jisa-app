#!/usr/bin/env python3
"""
Complete RAG Chatbot Pipeline for HO&F JISA App:
1. User Query â†’ Gemini Flash (query enhancement with metadata_key.json)
2. Enhanced Query â†’ Pinecone (retrieve top 4 results)
3. Retrieved Context â†’ Gemini 2.5 Pro (generate final answer)
"""

import os
import json
from pathlib import Path
from dotenv import load_dotenv
from pinecone import Pinecone
from openai import OpenAI
from google import genai

load_dotenv()

# Initialize clients
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
genai_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# Constants
INDEX_NAME = "hof-branch-chatbot"
NAMESPACE = "hof-knowledge-base-max"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# Get the directory where this script is located
SCRIPT_DIR = Path(__file__).parent
METADATA_KEY_PATH = SCRIPT_DIR / "metadata_key.json"
PDF_URLS_PATH = SCRIPT_DIR / "pdf_urls.json"


def load_metadata_key():
    """Load the metadata key for context."""
    with open(METADATA_KEY_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def load_pdf_urls():
    """Load PDF URLs configuration."""
    with open(PDF_URLS_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def get_relevant_pdfs(user_query: str, results) -> list:
    """
    Determine which PDFs to attach based on query and results.

    Args:
        user_query: User's question
        results: Pinecone search results

    Returns:
        List of PDF objects to attach
    """
    pdf_config = load_pdf_urls()
    relevant_pdfs = []

    # Check if query is about schedules/training/education
    schedule_keywords = ['ì¼ì •', 'ìŠ¤ì¼€ì¤„', 'êµìœ¡', 'ê°•ì˜', 'ì‹œí—˜', 'í–‰ì‚¬', 'KRS', 'ì…ë¬¸ê³¼ì •', 'ì‹œê°„í‘œ']
    is_schedule_query = any(keyword in user_query for keyword in schedule_keywords)

    # Check if query is about Hanwha commissions/policies
    hanwha_keywords = ['í•œí™”ìƒëª…', 'í•œí™”', 'ì‹œì±…', 'ìˆ˜ìˆ˜ë£Œ', 'ì»¤ë¯¸ì…˜', 'ìµì›”', '13ì°¨ì›”']
    is_hanwha_query = any(keyword in user_query for keyword in hanwha_keywords)

    # Check results for schedule or Hanwha data
    has_schedule_results = False
    has_hanwha_results = False

    if results.matches:
        for match in results.matches[:5]:  # Check top 5 results
            chunk_type = match.metadata.get('chunk_type', '')
            if chunk_type in ['event_individual', 'day_summary', 'event_range']:
                has_schedule_results = True
            if chunk_type in ['table_cell_commission', 'table_row_summary', 'table_column_summary']:
                has_hanwha_results = True

    # Add schedule PDFs if relevant
    if is_schedule_query or has_schedule_results:
        # Add main schedule PDF
        relevant_pdfs.append(pdf_config['schedule_pdfs'][0])  # 24ë…„ í˜¸ì•¤ì—í”„ì§€ì‚¬ ì¼ì •í‘œ

        # Add KRS PDF if KRS-related
        if 'KRS' in user_query or 'krs' in user_query.lower() or 'ì…ë¬¸' in user_query:
            relevant_pdfs.append(pdf_config['schedule_pdfs'][1])  # KRS ì‹œê°„í‘œ

    # Add policy PDFs if relevant
    if is_hanwha_query or has_hanwha_results:
        relevant_pdfs.append(pdf_config['policy_pdfs'][0])  # í•œí™”ìƒëª… ì‹œì±…ê³µì§€

    return relevant_pdfs


def format_pdf_attachments(pdfs: list) -> str:
    """Format PDF attachments for inclusion in response."""
    if not pdfs:
        return ""

    attachment_text = "\n\n" + "â”€" * 60 + "\n"
    attachment_text += "ğŸ“ **ì°¸ê³  ìë£Œ**\n\n"

    for pdf in pdfs:
        attachment_text += f"**{pdf['description']}**\n"
        attachment_text += f"ğŸ”— [PDF ë³´ê¸°]({pdf['url']})\n\n"

    return attachment_text


def enhance_query_with_gemini_flash(user_query: str, metadata_key: dict) -> dict:
    """
    Step 1: Use Gemini Flash to enhance query and generate Pinecone filters.
    Uses gemini-flash-latest for fast query optimization with metadata context.
    """

    # Instructions for Hanwha commission queries (now in main namespace)
    hanwha_instructions = """
## HANWHA COMMISSION QUERIES (í•œí™”ìƒëª… 11ì›” ì‹œì±… - ì´ˆì„¸ë°€ ë°ì´í„°)

**ì´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” 264ê°œì˜ ì´ˆì„¸ë°€ ë²¡í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:**

### ì‚¬ìš© ê°€ëŠ¥í•œ Chunk Types:
- **table_cell_commission** (154ê°œ): ê°œë³„ ìƒí’ˆì˜ íŠ¹ì • ìˆ˜ìˆ˜ë£Œìœ¨ (ê°€ì¥ ì„¸ë°€í•¨)
  * ê° ì…€ë§ˆë‹¤ ë…ë¦½ì ì¸ ë²¡í„°
  * ìƒí’ˆëª…, ë‚©ê¸°, ìˆ˜ìˆ˜ë£Œ ìœ í˜•(ì¢…í•©/1ì°¨ì‹œì±…/2ì°¨ì‹œì±…), ê¸°ê°„(ìµì›”/13ì°¨ì›”)ìœ¼ë¡œ í•„í„°ë§ ê°€ëŠ¥

- **table_row_summary** (26ê°œ): í•œ ìƒí’ˆì˜ ëª¨ë“  ìˆ˜ìˆ˜ë£Œìœ¨ ìš”ì•½

- **table_column_summary** (35ê°œ): í•œ ìˆ˜ìˆ˜ë£Œ ìœ í˜•ì˜ ëª¨ë“  ìƒí’ˆ ëª©ë¡

- **page_full** (5ê°œ): ì „ì²´ í˜ì´ì§€ ì»¨í…ì¸ 

- **heading_with_context** (7ê°œ): ì„¹ì…˜ í—¤ë”ì™€ ì„¤ëª…

- **text_sentence_group** (31ê°œ): ì •ì±… ë° ê·œì • ìƒì„¸ ì„¤ëª…

### í•„í„°ë§ ì „ëµ (CRITICAL):
1. **íŠ¹ì • ìƒí’ˆ + íŠ¹ì • ìˆ˜ìˆ˜ë£Œ** ì§ˆë¬¸:
   - chunk_type: "table_cell_commission" í•„ìˆ˜
   - product_name_clean: ìƒí’ˆëª… (ì •í™•í•œ ë§¤ì¹­)
   - commission_category OR commission_period ì‚¬ìš©

2. **ìƒí’ˆ ì „ì²´ ìˆ˜ìˆ˜ë£Œ** ì§ˆë¬¸:
   - chunk_type: "table_row_summary"

3. **ìˆ˜ìˆ˜ë£Œ ìœ í˜•ë³„ ì „ì²´ ìƒí’ˆ** ì§ˆë¬¸:
   - chunk_type: "table_column_summary"

4. **ì •ì±…/í™˜ìˆ˜/ê·œì •** ì§ˆë¬¸:
   - chunk_type: "text_sentence_group"

### ë©”íƒ€ë°ì´í„° í•„ë“œ (Hanwha ì „ìš©):
- product_name: ì›ë³¸ ìƒí’ˆëª… (ì˜ˆ: "(ì¼ë°˜/ê°„í¸) ë ˆì´ë””Hë³´ì¥ë³´í—˜")
- product_name_clean: ì •ì œëœ ìƒí’ˆëª… (ì˜ˆ: "ë ˆì´ë””Hë³´ì¥ë³´í—˜")
- payment_term: ë‚©ê¸° (ì˜ˆ: "10ë…„ë‚©", "20ë…„ë‚©â†‘", "ë‚©ê¸°ë¬´ê´€")
- commission_type: ìˆ˜ìˆ˜ë£Œ í‚¤ (comprehensive_current, fc_policy_13th, ë“±)
- commission_label: ìˆ˜ìˆ˜ë£Œ ë¼ë²¨ (ì˜ˆ: "ì¢…í•© ìµì›”", "1ì°¨ì‹œì±…(FCì‹œì±…) 13ì°¨ì›”")
- commission_value: ì‹¤ì œ ìˆ˜ìˆ˜ë£Œìœ¨ (ì˜ˆ: "485.0%", "194.0%")
- commission_category: ì¹´í…Œê³ ë¦¬ ("ì¢…í•©", "1ì°¨ì‹œì±…(FCì‹œì±…)", "2ì°¨ì‹œì±…(ë³¸ë¶€ì‹œì±…)")
- commission_period: ê¸°ê°„ ("ìµì›”", "13ì°¨ì›”")

### Boolean í•„í„° (Hanwha):
- is_current_month: Trueë©´ ìµì›” ìˆ˜ìˆ˜ë£Œ
- is_13th_month: Trueë©´ 13ì°¨ì›” ìˆ˜ìˆ˜ë£Œ
- is_fc_policy: Trueë©´ FCì‹œì±…
- is_hq_policy: Trueë©´ ë³¸ë¶€ì‹œì±…
- is_comprehensive: Trueë©´ ì¢…í•©

### ì¿¼ë¦¬ ìµœì í™” ì˜ˆì‹œ:
1. "ë ˆì´ë””Hë³´ì¥ë³´í—˜ ì¢…í•© ìµì›” ìˆ˜ìˆ˜ë£Œ"
   â†’ filter: {{"chunk_type": "table_cell_commission", "is_comprehensive": true, "is_current_month": true}}
   â†’ enhanced_query: "ë ˆì´ë””Hë³´ì¥ë³´í—˜ ì¢…í•© ìµì›” ìˆ˜ìˆ˜ë£Œìœ¨"

2. "ì œë¡œë°±Hì¢…ì‹  20ë…„ë‚©"
   â†’ filter: {{"chunk_type": "table_cell_commission", "payment_term": "20ë…„ë‚©"}}
   â†’ enhanced_query: "ì œë¡œë°±Hì¢…ì‹  20ë…„ë‚© ìˆ˜ìˆ˜ë£Œ"

3. "Hê±´ê°•í”ŒëŸ¬ìŠ¤ ëª¨ë“  ìˆ˜ìˆ˜ë£Œ"
   â†’ filter: {{"chunk_type": "table_row_summary"}}
   â†’ enhanced_query: "Hê±´ê°•í”ŒëŸ¬ìŠ¤ ì „ì²´ ìˆ˜ìˆ˜ë£Œìœ¨"

### CRITICAL FILTERING RULES for Hanwha:
1. **NEVER use product_name or product_name_clean in filters** - semantic search will find products!
2. **NEVER use companies field** - it doesn't exist! Field is "company" but don't filter by it
3. **ONLY use these fields**:
   - chunk_type (REQUIRED: "table_cell_commission" or "table_row_summary" or "table_column_summary")
   - Boolean flags: is_comprehensive, is_current_month, is_13th_month, is_fc_policy, is_hq_policy
   - payment_term (ONLY if user explicitly says "20ë…„ë‚©", "10ë…„ë‚©", etc.)
4. **Semantic search handles product matching** automatically via searchable_text field
5. **Example good filter**: {{"chunk_type": "table_cell_commission", "is_comprehensive": true, "is_current_month": true}}
6. **Example BAD filter**: {{"product_name": "...", "companies": [...], "product_name_clean": "..."}} â† DON'T DO THIS!

## SCHEDULE QUERIES (ì¼ì •, êµìœ¡, ì‹œí—˜ - ì´ˆì„¸ë°€ ë°ì´í„°)

**ì´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” 91ê°œì˜ ì´ˆì„¸ë°€ ìŠ¤ì¼€ì¤„ ë²¡í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤:**

### ì‚¬ìš© ê°€ëŠ¥í•œ Chunk Types:
- **event_individual** (47ê°œ): ê°œë³„ êµìœ¡/í–‰ì‚¬ (ë‚ ì§œ, ì‹œê°„, ê°•ì‚¬ í¬í•¨)
- **day_summary** (14ê°œ): ì¼ì¼ ì „ì²´ ì¼ì • ìš”ì•½
- **event_range** (30ê°œ): ê¸°ê°„ í–‰ì‚¬ (ìœ„ì´‰ ë§í¬, ì‹œí—˜ ê¸°ê°„ ë“±)

### Schedule í•„í„°ë§ ì „ëµ (CRITICAL - USE CONSERVATIVE FILTERS):

**IMPORTANT: For schedule queries, use MINIMAL filters to avoid missing data!**

1. **íŠ¹ì • ë‚ ì§œ ì§ˆë¬¸** ("11ì›” 4ì¼ ì¼ì •", "4ì¼ì— ë­ ìˆì–´?"):
   - Use: {{"date_start": "2025-11-04"}} OR {{"date": "2025-11-04"}}
   - chunk_type: "event_individual" OR "day_summary"
   - DO NOT add is_training filter unless explicitly asked!

2. **êµìœ¡ ê´€ë ¨** ("êµìœ¡ ì¼ì •", "ê°•ì˜ ìŠ¤ì¼€ì¤„"):
   - Use ONLY: {{"is_training": true}}
   - DO NOT use date filters unless date is explicitly mentioned!

3. **ì‹œí—˜ ê´€ë ¨** ("ì‹œí—˜ ì¼ì •", "ìƒëª…ë³´í—˜ ì‹œí—˜"):
   - Use: {{"is_exam": true}}
   - Optionally add chunk_type: "event_range" for exam periods

4. **ê°•ì‚¬/ì¥ì†Œ ê´€ë ¨**:
   - Let semantic search handle it - DO NOT filter!
   - Enhanced query should include presenter/location terms

### Schedule Boolean í•„í„°:
- is_training: êµìœ¡/ê°•ì˜/ê³¼ì •
- is_exam: ì‹œí—˜/ì‘ì‹œ
- is_appointment: ìœ„ì´‰/ì½”ë“œë°œê¸‰
- is_deadline: ë§ˆê°/ì ‘ìˆ˜
- is_ceremony: ìˆ˜ë£Œì‹
- is_orientation: ì˜¤ë¦¬ì—”í…Œì´ì…˜
- is_partner_education: ì œíœ´ì‚¬ êµìœ¡
- is_kblp: KBLP ë³¸ì‚¬ ê°•ì˜
- is_zoom: ZOOM ê°•ì˜
- is_conference: Conference

### ì˜ˆì‹œ (FOLLOW THESE PATTERNS):
1. "11ì›” 4ì¼ ê°•ì˜ ìŠ¤ì¼€ì¤„"
   â†’ filter: {{"date_start": "2025-11-04"}}  # NO is_training! Date is specific enough
   â†’ enhanced_query: "11ì›” 4ì¼ ê°•ì˜ êµìœ¡ ì¼ì • ìŠ¤ì¼€ì¤„"

2. "êµìœ¡ ì¼ì •"
   â†’ filter: {{"is_training": true}}  # ONLY boolean flag
   â†’ enhanced_query: "êµìœ¡ ê°•ì˜ íŠ¸ë ˆì´ë‹ ì¼ì •"

3. "ì‚¼ì„±í™”ì¬ êµìœ¡"
   â†’ filter: {{"is_training": true}}  # Let semantic search find company
   â†’ enhanced_query: "ì‚¼ì„±í™”ì¬ ì œíœ´ì‚¬ êµìœ¡"

4. "11ì›” 7ì¼ ì‹œí—˜"
   â†’ filter: {{"$or": [{{"date_start": "2025-11-07"}}, {{"date": "2025-11-07"}}]}}
   â†’ enhanced_query: "11ì›” 7ì¼ ì‹œí—˜ ì‘ì‹œ ìƒëª…ë³´í—˜"

**CRITICAL RULES:**
1. **NEVER use $gte, $lte, $gt, $lt with date strings** - Pinecone only supports these with numbers!
2. **For date ranges, use $eq ONLY** - semantic search will handle date proximity
3. Prefer semantic search over strict filters for schedules!
4. Only use date filters when date is explicitly mentioned.
"""

    prompt = f"""You are an expert query optimizer for a Korean insurance branch office RAG system.

{hanwha_instructions}

## AVAILABLE METADATA IN PINECONE:

**IMPORTANT: These are EXAMPLES only. Semantic search handles actual matching!**

**Chunk Types (select based on query type):** {', '.join(metadata_key.get('chunk_types', []))}
**Content Types (examples):** {', '.join(metadata_key.get('content_types', []))}
**Primary Categories (examples):** {', '.join(metadata_key.get('primary_categories', []))}
**Insurance Companies:** {', '.join(metadata_key.get('companies', []))}
**Product Examples:** {', '.join(metadata_key.get('product_names_examples', [])[:5])}...
**Presenter Examples:** {', '.join(metadata_key.get('presenters_examples', [])[:4])}...
**Locations:** {', '.join(metadata_key.get('locations', []))}
**Payment Terms:** {', '.join(metadata_key.get('payment_terms', [])[:8])}...
**Commission Categories:** {', '.join(metadata_key.get('commission_categories', []))}
**Commission Periods:** {', '.join(metadata_key.get('commission_periods', []))}

## BOOLEAN FLAGS AVAILABLE:
{chr(10).join(f"- {flag}" for flag in metadata_key.get('boolean_filters', []))}

## USER QUERY:
"{user_query}"

## YOUR TASK:
Generate optimized search query and Pinecone filters.

## PRODUCT NAME MATCHING RULES:
- When user asks about a product (ë³´í—˜, ìƒí’ˆ), use PARTIAL matching, not exact
- Remove qualifiers like "ì¼ë°˜/", "ê°„í¸/", "(ì¼ë°˜)", "(ê°„í¸)" when creating filters
- Example: "Hê±´ê°•í”ŒëŸ¬ìŠ¤" should match "Hê±´ê°•í”ŒëŸ¬ìŠ¤", "(ì¼ë°˜/ê°„í¸) Hê±´ê°•í”ŒëŸ¬ìŠ¤", "ê°„í¸ Hê±´ê°•í”ŒëŸ¬ìŠ¤"
- Use semantic search (enhanced_query) for product names, NOT strict filters
- Only use product filters when user specifies exact product codes or very specific names

## DATE EXTRACTION RULES (HIGHEST PRIORITY - FOLLOW EXACTLY):

**CRITICAL: NEVER use $gte/$lte with date strings! Only use $eq.**

### Rule 1: Specific Date (ë‚ ì§œ ëª…ì‹œ)
If query contains: "Nì›” Mì¼", "Mì¼", "N/M", or day number:
- MUST use: {{"date_start": {{"$eq": "2025-11-04"}}}} OR {{"date": {{"$eq": "2025-11-04"}}}}
- Examples that trigger this:
  * "11ì›” 4ì¼ í–‰ì‚¬" â†’ {{"$or": [{{"date_start": "2025-11-04"}}, {{"date": "2025-11-04"}}]}}
  * "4ì¼ì— ë­ ìˆì–´?" â†’ {{"$or": [{{"date_start": "2025-11-04"}}, {{"date": "2025-11-04"}}]}}

### Rule 2: Date Range (ê¸°ê°„) - DO NOT USE FILTERS!
If query contains: "Nì¼ë¶€í„° Mì¼", "Nì¼~Mì¼":
- DO NOT use filters! Let semantic search handle it
- Enhanced query should include both dates

### Rule 3: Month Only (ì›”ë§Œ ì–¸ê¸‰)
ONLY if query has month but NO specific day:
- Use: {{"month": {{"$eq": "2025-11"}}}}
- Examples: "11ì›” ì¼ì •", "11ì›”ì— ë­ ìˆì–´?"

### Rule 4: Date Format
- Always use YYYY-MM-DD format
- Current year is 2025
- Convert Korean dates: "11ì›” 4ì¼" â†’ "2025-11-04"

## WHEN NOT TO FILTER (CRITICAL):

**Use NO FILTERS (null) for these query types:**
1. General procedures/ì ˆì°¨ questions (ë³´ì¦ë³´í—˜ ë™ì˜ ì ˆì°¨, ê°€ì… ë°©ë²•, etc.)
2. General "how to" questions (ì–´ë–»ê²Œ, ë°©ë²•, ì•ˆë‚´)
3. **Resource/link/material requests** (ë§í¬, ìë£Œ, ë¬¸ì„œ, íŒŒì¼, ìˆì–´?, ëª¨ìŒ) â† MOST IMPORTANT!
4. Policy/regulation questions WITHOUT specific keywords
5. When you're unsure - prefer semantic search over strict filters!

**SPECIAL RULE for ìë£Œ/ë§í¬/íŒŒì¼ queries:**
- When user asks "ë£¨í‚¤ ìŠ¤ì¿¨ ìë£Œ ìˆì–´?", "êµìœ¡ ìë£Œ ë§í¬", "íŒŒì¼ ìˆì–´?" â†’ **NEVER use filters!**
- Resource documents are often category="resource_links" or "link", NOT "is_training"
- Filters will EXCLUDE the actual resource links you're looking for!
- Let semantic search find the right materials

**Only use filters when:**
- User asks for specific date ("11ì›” 4ì¼ ì¼ì •")
- User asks for specific schedule type (êµìœ¡ ì¼ì •, ì‹œí—˜ ì¼ì •) â† schedule, not materials!
- User asks about Hanwha commissions (use chunk_type)
- User specifies boolean criteria explicitly

**Examples - NO FILTER:**
- "ë³´ì¦ë³´í—˜ ë™ì˜ ì ˆì°¨ ë­ì•¼?" â†’ filters: null
- "ì–´ë–»ê²Œ ê°€ì…í•´?" â†’ filters: null
- "ìë£Œ ë§í¬ ì•Œë ¤ì¤˜" â†’ filters: null â† Resource request
- "ë£¨í‚¤ ìŠ¤ì¿¨ ìë£Œ ìˆì–´?" â†’ filters: null â† Resource request
- "êµìœ¡ ìë£Œ íŒŒì¼" â†’ filters: null â† Resource request
- "KRS ë§í¬" â†’ filters: null â† Resource request

**Examples - USE FILTER:**
- "11ì›” 4ì¼ êµìœ¡ ì¼ì •" â†’ filters: {{"date": "2025-11-04", "is_training": true}} â† Schedule query
- "ì´ë²ˆ ì£¼ ì‹œí—˜ ì¼ì •" â†’ filters: {{"is_exam": true}} â† Schedule query

## FILTER SYNTAX RULES:
- Use {{"field": {{"$eq": value}}}} for exact match
- Use {{"field": {{"$in": [values]}}}} for arrays
- **NEVER use $gte, $lte, $gt, $lt with STRINGS** - only use with numbers!
- For date strings, ONLY use $eq - no comparison operators!
- Use {{"$and": [conditions]}} for multiple conditions (all must match)
- Use {{"$or": [conditions]}} for alternative conditions (any can match)
- Boolean flags use true/false (lowercase)

## EXACT FIELD NAMES:
- companies (array)
- products (array)
- locations (array)
- month (string, format: "2025-11")
- date (string, format: "2025-11-04")
- date_start (string, format: "2025-11-04")
- date_end (string, format: "2025-11-06")
- content_type (string)
- primary_category (string)
- sub_category (string)
- semantic_tags (array)
- keywords (array)
- is_training, is_exam, is_promotion, is_policy, etc. (boolean)

## OUTPUT FORMAT (VALID JSON ONLY):
```json
{{
  "enhanced_query": "optimized Korean search text with core terms and variations",
  "filters": {{
    // Pinecone filter object, or null if no filters needed
  }},
  "reasoning": "Brief explanation"
}}
```

**Enhanced Query Tips:**
- For products: include base name + variations (ì˜ˆ: "Hê±´ê°•í”ŒëŸ¬ìŠ¤ OR Hê±´ê°•í”ŒëŸ¬ìŠ¤ìƒí’ˆ OR ê°„í¸Hê±´ê°•í”ŒëŸ¬ìŠ¤ OR ì¼ë°˜Hê±´ê°•í”ŒëŸ¬ìŠ¤")
- For dates: include multiple formats (ì˜ˆ: "11ì›” 4ì¼ OR 11/4 OR 4ì¼")
- Use semantic expansions to improve recall

Return ONLY valid JSON, no markdown.
"""

    response = genai_client.models.generate_content(
        model='gemini-flash-latest',
        contents=prompt
    )
    response_text = response.text.strip()

    # Clean markdown
    if response_text.startswith("```json"):
        response_text = response_text[7:]
    if response_text.startswith("```"):
        response_text = response_text[3:]
    if response_text.endswith("```"):
        response_text = response_text[:-3]

    try:
        return json.loads(response_text.strip())
    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSON Parse Error: {e}")
        return {
            "enhanced_query": user_query,
            "filters": None,
            "reasoning": "Failed to parse Gemini response"
        }


def get_embedding(text: str):
    """Generate embedding for query text."""
    response = openai_client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text,
        dimensions=EMBEDDING_DIMENSIONS
    )
    return response.data[0].embedding


def retrieve_from_pinecone(enhanced_query: str, filters: dict = None, top_k: int = 4):
    """
    Step 2: Query Pinecone with enhanced query and filters.

    Args:
        enhanced_query: Optimized search query
        filters: Pinecone metadata filters
        top_k: Number of results to retrieve
    """
    index = pc.Index(INDEX_NAME)

    # Generate embedding
    query_embedding = get_embedding(enhanced_query)

    # Query Pinecone
    results = index.query(
        vector=query_embedding,
        top_k=top_k,
        namespace=NAMESPACE,
        include_metadata=True,
        filter=filters
    )

    return results


def format_context_for_gemini(results) -> str:
    """
    Format Pinecone results into context for Gemini 2.5 Pro.
    Handles both general and Hanwha-specific metadata.
    """
    if not results.matches:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    context_parts = []

    for idx, match in enumerate(results.matches, 1):
        meta = match.metadata
        chunk_type = meta.get('chunk_type', 'N/A')

        # Check if this is Hanwha commission data
        is_hanwha = chunk_type in ['table_cell_commission', 'table_row_summary', 'table_column_summary']

        # Check if this is schedule data
        is_schedule = chunk_type in ['event_individual', 'day_summary', 'event_range']

        if is_schedule:
            # Format schedule-specific data
            context = f"""
## ë¬¸ì„œ {idx} (ê´€ë ¨ë„: {match.score:.3f})

**ì¶œì²˜:** {meta.get('source_file', 'Schedule')}
**ìœ í˜•:** {chunk_type}
"""
            # Event details
            if meta.get('title'):
                context += f"**ì œëª©:** {meta.get('title')}\n"

            # Date information
            if chunk_type == 'event_individual' or chunk_type == 'day_summary':
                date = meta.get('date', '')
                weekday = meta.get('weekday', '')
                if date:
                    context += f"**ë‚ ì§œ:** {date}"
                    if weekday:
                        context += f" ({weekday})"
                    context += "\n"
            elif chunk_type == 'event_range':
                date_start = meta.get('date_start', '')
                date_end = meta.get('date_end', '')
                duration = meta.get('duration_days', 0)
                if date_start and date_end:
                    context += f"**ê¸°ê°„:** {date_start} ~ {date_end}"
                    if duration:
                        context += f" ({duration}ì¼ê°„)"
                    context += "\n"
                if meta.get('business_days'):
                    context += f"**ì˜ì—…ì¼:** {meta.get('business_days')}ì¼\n"

            # Time, Location, Presenter
            if meta.get('time'):
                context += f"**ì‹œê°„:** {meta.get('time')}\n"
            if meta.get('location'):
                context += f"**ì¥ì†Œ:** {meta.get('location')}\n"
            if meta.get('presenter'):
                context += f"**ê°•ì‚¬:** {meta.get('presenter')}\n"
            if meta.get('category'):
                context += f"**ì¹´í…Œê³ ë¦¬:** {meta.get('category')}\n"

            # Companies/Regions
            if meta.get('companies'):
                context += f"**ë³´í—˜ì‚¬:** {', '.join(meta.get('companies', []))}\n"
            if meta.get('regions'):
                context += f"**ì§€ì—­:** {', '.join(meta.get('regions', []))}\n"

            # Event count for day summaries
            if chunk_type == 'day_summary' and meta.get('event_count'):
                context += f"**í–‰ì‚¬ ìˆ˜:** {meta.get('event_count')}ê°œ\n"
                if meta.get('event_titles'):
                    context += f"**í–‰ì‚¬ ëª©ë¡:** {', '.join(meta.get('event_titles', []))}\n"

            # Full content
            searchable = meta.get('searchable_text', meta.get('natural_description', ''))
            if searchable:
                context += f"\n**ìƒì„¸ ë‚´ìš©:**\n{searchable}\n"

        elif is_hanwha:
            # Format Hanwha-specific data
            context = f"""
## ë¬¸ì„œ {idx} (ê´€ë ¨ë„: {match.score:.3f})

**ì¶œì²˜:** í•œí™”ìƒëª… 11ì›” ì‹œì±…ê³µì§€
**ìœ í˜•:** {chunk_type}
"""
            if chunk_type == 'table_cell_commission':
                context += f"**ìƒí’ˆëª…:** {meta.get('product_name', 'N/A')}\n"
                if meta.get('payment_term'):
                    context += f"**ë‚©ê¸°:** {meta.get('payment_term')}\n"
                context += f"**ì‹œì±… ìœ í˜•:** {meta.get('commission_label', 'N/A')}\n"
                context += f"**ìˆ˜ìˆ˜ë£Œìœ¨:** {meta.get('commission_value', 'N/A')}\n"
                context += f"**ì¹´í…Œê³ ë¦¬:** {meta.get('commission_category', 'N/A')}\n"
                context += f"**ê¸°ê°„:** {meta.get('commission_period', 'N/A')}\n"

            elif chunk_type == 'table_row_summary':
                context += f"**ìƒí’ˆëª…:** {meta.get('product_name', 'N/A')}\n"
                if meta.get('payment_term'):
                    context += f"**ë‚©ê¸°:** {meta.get('payment_term')}\n"
                rates = meta.get('all_commission_values', [])
                if rates:
                    context += f"**ì „ì²´ ìˆ˜ìˆ˜ë£Œìœ¨:** {', '.join(str(r) for r in rates)}\n"

            elif chunk_type == 'table_column_summary':
                context += f"**ì‹œì±… ìœ í˜•:** {meta.get('column_header', 'N/A')}\n"
                context += f"**ìƒí’ˆ ê°œìˆ˜:** {meta.get('product_count', 0)}ê°œ\n"

            # Add searchable text
            searchable = meta.get('searchable_text', meta.get('natural_description', ''))
            if searchable:
                context += f"\n**ìƒì„¸ ë‚´ìš©:**\n{searchable}\n"

        else:
            # Format general data (original format)
            context = f"""
## ë¬¸ì„œ {idx} (ê´€ë ¨ë„: {match.score:.3f})

**ì œëª©:** {meta.get('title', 'N/A')}
**ì¶œì²˜:** {meta.get('source_file', meta.get('doc_title', 'N/A'))}
**ìœ í˜•:** {meta.get('content_type', meta.get('type', meta.get('doc_type', 'N/A')))}
**ì¹´í…Œê³ ë¦¬:** {meta.get('primary_category', meta.get('category', 'N/A'))} â†’ {meta.get('sub_category', 'N/A')}
"""

            # Add relevant metadata for general documents
            if meta.get('insurance_company'):
                context += f"**ë³´í—˜ì‚¬:** {meta.get('insurance_company')}\n"
            if meta.get('company'):
                context += f"**íšŒì‚¬:** {meta.get('company')}\n"
            if meta.get('provider'):
                context += f"**ì œê³µ:** {meta.get('provider')}\n"
            if meta.get('date'):
                context += f"**ë‚ ì§œ:** {meta.get('date')}\n"
            if meta.get('date_start') and meta.get('date_end'):
                context += f"**ê¸°ê°„:** {meta.get('date_start')} ~ {meta.get('date_end')}\n"
            if meta.get('payout_amount'):
                context += f"**ì§€ì›ê¸ˆ:** {meta.get('payout_amount'):,.0f}ì›\n"
            if meta.get('financial_tier'):
                context += f"**ê¸ˆì•¡êµ¬ê°„:** {meta.get('financial_tier')}\n"
            if meta.get('people'):
                context += f"**ê´€ë ¨ì¸ë¬¼:** {', '.join(meta.get('people', []))}\n"
            if meta.get('locations'):
                context += f"**ì¥ì†Œ:** {', '.join(meta.get('locations', []))}\n"
            if meta.get('products'):
                context += f"**ìƒí’ˆ:** {', '.join(meta.get('products', [])[:5])}\n"

            # Extract URLs/links (universal for all document types)
            url = meta.get('url') or meta.get('app_link') or meta.get('link') or meta.get('resource_url')
            if url:
                context += f"**ë§í¬:** {url}\n"

            # For insurance_procedures type, extract structured info
            if meta.get('category') == 'insurance_procedures' or meta.get('type') == 'guarantee_insurance_consent':
                if meta.get('procedure_steps'):
                    context += f"**ì ˆì°¨:** {meta.get('procedure_steps')}\n"
                if meta.get('required_info'):
                    context += f"**í•„ìš”ì •ë³´:** {meta.get('required_info')}\n"
                if meta.get('important_note'):
                    context += f"**ì¤‘ìš”:** {meta.get('important_note')}\n"
                if meta.get('warning'):
                    context += f"**ì£¼ì˜ì‚¬í•­:** {meta.get('warning')}\n"
                if meta.get('purpose_detail'):
                    context += f"**ëª©ì :** {meta.get('purpose_detail')}\n"

            # For resource_links type, add description
            if meta.get('category') == 'resource_links':
                if meta.get('doc_type'):
                    context += f"**ìë£Œ ìœ í˜•:** {meta.get('doc_type')}\n"
                if meta.get('keywords'):
                    context += f"**í‚¤ì›Œë“œ:** {meta.get('keywords')}\n"

            # For zoom meetings, add meeting details
            if meta.get('category') == 'zoom_meeting':
                if meta.get('meeting_id'):
                    context += f"**Meeting ID:** {meta.get('meeting_id')}\n"
                if meta.get('passcode'):
                    context += f"**Passcode:** {meta.get('passcode')}\n"

            # Full text content - try multiple field names
            text_content = meta.get('full_text') or meta.get('text') or meta.get('text_preview') or meta.get('searchable_text') or 'N/A'
            if text_content and text_content != 'N/A':
                context += f"\n**ì „ì²´ ë‚´ìš©:**\n{text_content}\n"

        context_parts.append(context)

    return "\n".join(context_parts)


def detect_question_type(user_query: str) -> str:
    """
    Detect the type of question to select appropriate prompt strategy.

    Returns:
        'list_all': User wants comprehensive list of all items (í–‰ì‚¬, êµìœ¡, í”„ë¡œëª¨ì…˜, etc.)
        'specific': User wants specific information about one thing
        'explanation': User wants explanation or understanding
        'single': User explicitly wants one item only
    """
    query_lower = user_query.lower()

    # Keywords indicating "show me everything"
    list_all_keywords = ['ëª¨ë‘', 'ì „ë¶€', 'ë‹¤', 'ì „ì²´', 'ëª¨ë“ ', 'ëª‡', 'ë­', 'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ']

    # Keywords indicating "just one" or "specific item"
    single_keywords = ['í•˜ë‚˜ë§Œ', 'ì²«ë²ˆì§¸', 'ì²« ë²ˆì§¸', 'ê°€ì¥', 'ì œì¼', 'ìµœê³ ', 'ì£¼ìš”í•œ', 'ì¤‘ìš”í•œ']

    # Question types that expect lists
    list_context_words = ['í–‰ì‚¬', 'êµìœ¡', 'ì¼ì •', 'í”„ë¡œëª¨ì…˜', 'ì‹œì±…', 'ì›Œí¬ìƒµ', 'ì„¸ë¯¸ë‚˜', 'ê°•ì˜', 'ë¯¸íŒ…']

    # Check for explicit "single item" request
    if any(kw in query_lower for kw in single_keywords):
        return 'single'

    # Check for list-all request with context words
    has_list_keyword = any(kw in query_lower for kw in list_all_keywords)
    has_list_context = any(kw in query_lower for kw in list_context_words)

    if has_list_context and has_list_keyword:
        return 'list_all'

    # If asking "what events" type questions without limiting words
    if has_list_context:
        return 'list_all'

    # Default to explanation for other types
    return 'explanation'


def generate_answer_with_gemini_pro(user_query: str, context: str) -> str:
    """
    Step 3: Use Gemini 2.5 Pro to generate final answer based on retrieved context.
    Uses gemini-2.5-pro for high-quality final inference.
    Selects specialized prompt based on question type.
    """

    question_type = detect_question_type(user_query)
    print(f"   ğŸ¯ ì§ˆë¬¸ ìœ í˜•: {question_type}")

    # Base formatting instructions (shared across all prompts)
    formatting_instructions = """
íŠ¹ë³„ ì§€ì¹¨ (ì¶œë ¥ í˜•ì‹):
- ë§ˆí¬ë‹¤ìš´ ì‚¬ìš© ê¸ˆì§€: **, ##, *, -, [], () ë“± ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©: ë“¤ì—¬ì“°ê¸°(ìŠ¤í˜ì´ìŠ¤)ì™€ ì¤„ë°”ê¿ˆë§Œìœ¼ë¡œ êµ¬ì¡°ë¥¼ í‘œí˜„í•˜ì„¸ìš”
- ëª©ë¡ í‘œì‹œ: ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ëŒ€ì‹  "1. ", "2. " ë˜ëŠ” "- " ê°™ì€ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œí•˜ì„¸ìš”
- ê°•ì¡° í‘œì‹œ: ë³„í‘œë‚˜ ë°‘ì¤„ ëŒ€ì‹  ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë¡œ ì‘ì„±í•˜ì„¸ìš”
- í‘œ ê¸ˆì§€: í‘œ í˜•ì‹ ëŒ€ì‹  í…ìŠ¤íŠ¸ë¡œ ë‚˜ì—´í•˜ì„¸ìš”
- ê¸ˆì•¡ì€ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš” (ì˜ˆ: 1,000,000ì›)
- ë‚ ì§œëŠ” ëª…í™•í•˜ê²Œ í‘œì‹œí•˜ì„¸ìš” (ì˜ˆ: 2025ë…„ 11ì›” 1ì¼ ê¸ˆìš”ì¼ ~ 11ì›” 23ì¼ í† ìš”ì¼)
- ì‹œê°„ì€ ì˜¤ì „/ì˜¤í›„ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš” (ì˜ˆ: ì˜¤í›„ 2ì‹œ 30ë¶„)
- í”„ë¡œëª¨ì…˜/ì‹œì±…ì€ ì ìš© ì¡°ê±´ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”
- êµìœ¡ ì¼ì •ì€ ë‚ ì§œ, ìš”ì¼, ì‹œê°„, ì¥ì†Œ, ê°•ì‚¬ë¥¼ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
- í™˜ìˆ˜ ê·œì •ì€ íšŒì°¨ë³„ë¡œ ë‚˜ì—´í•˜ì„¸ìš” (ì˜ˆ: 1íšŒì°¨ í™˜ìˆ˜ìœ¨ 100%, 2íšŒì°¨ í™˜ìˆ˜ìœ¨ 80%)
"""

    # Commission-specific instructions
    commission_instructions = """
**ìˆ˜ìˆ˜ë£Œ ë°ì´í„° ì²˜ë¦¬ ê·œì¹™ (CRITICAL):**

âŒ ì ˆëŒ€ ê¸ˆì§€:
- ì»¬ëŸ¼ ì´ë¦„ ì–¸ê¸‰ ê¸ˆì§€: col_8, col_19, col_20~col_43 ê°™ì€ ê¸°ìˆ  ìš©ì–´ ì‚¬ìš© ê¸ˆì§€
- ê³„ì‚°/ê³µì‹ ì–¸ê¸‰ ê¸ˆì§€: "ë°°ìœ¨", "ê³„ì‚°ì‹", "Ã—", "ê³µì‹" ì‚¬ìš© ê¸ˆì§€
- ë¶„ì„ ìš©ì–´ ê¸ˆì§€: "ë¶„í¬", "í•©ì‚°", "íŒ¨í„´", "êµ¬ê°„" ì‚¬ìš© ê¸ˆì§€
- ì†Œìˆ˜ì  í˜•ì‹ ê¸ˆì§€: 0.08148, 2.50842 ê°™ì€ ì†Œìˆ˜ ê·¸ëŒ€ë¡œ í‘œì‹œ ê¸ˆì§€
- ê¸°ìˆ  ì„¤ëª… ê¸ˆì§€: ë°ì´í„° êµ¬ì¡°, í…Œì´ë¸” êµ¬ì¡° ì„¤ëª… ê¸ˆì§€
- ì‹¤ë¬´ íŒ ê¸ˆì§€: "ì‹¤ë¬´ í™œìš© íŒ", "ë¹„êµ", "íŒë‹¨" ì¡°ì–¸ ê¸ˆì§€
- ìœ ì‚¬ ìƒí’ˆ ê¸ˆì§€: ë‹¤ë¥¸ ìƒí’ˆ ì¶”ì²œ ê¸ˆì§€

âœ… í•„ìˆ˜ ì²˜ë¦¬:
- ëª¨ë“  ì†Œìˆ˜ê°’ì€ Ã— 100í•˜ì—¬ ë°±ë¶„ìœ¨(%)ë¡œ ë³€í™˜
- ì˜ˆì‹œ: 0.405 â†’ 40.5%, 1.76346 â†’ 176.35%, 8.0 â†’ 800% (NOT 8.0!)
- ê°„ê²°í•˜ê²Œ: ìƒí’ˆëª…, íšŒì‚¬, ì£¼ìš” ìˆ˜ìˆ˜ë£Œìœ¨ë§Œ í‘œì‹œ
- ìˆëŠ” ì •ë³´ë§Œ: ì—†ëŠ” ì •ë³´ëŠ” "í•´ë‹¹ ì •ë³´ ì—†ìŒ"ì´ë¼ê³ ë§Œ í‘œì‹œ
"""

    if question_type == 'list_all':
        # USER WANTS COMPREHENSIVE LIST - SHOW EVERYTHING
        prompt = f"""ë‹¹ì‹ ì€ HO&F ì§€ì‚¬ AIì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ í–‰ì‚¬, êµìœ¡, ì¼ì • ë“±ì˜ ì™„ì „í•œ ëª©ë¡ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸:
{user_query}

ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ (ìµœëŒ€ 10ê°œ ë¬¸ì„œ):
{context}

**í•µì‹¬ ì§€ì¹¨ (ì ˆëŒ€ ì¤€ìˆ˜ - CRITICAL):**

âš ï¸ ë¬¸ì„œ íŒŒì‹± ê·œì¹™ (MOST IMPORTANT):
- ê²€ìƒ‰ëœ ë¬¸ì„œì— ì¼ì •í‘œ, ìº˜ë¦°ë”, ìŠ¤ì¼€ì¤„ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ë‚ ì§œì˜ **ëª¨ë“  í•­ëª©**ì„ ì¶”ì¶œí•˜ì„¸ìš”
- ë¬¸ì„œ ì•ˆì— ì—¬ëŸ¬ ë‚ ì§œ/ì´ë²¤íŠ¸ê°€ ë‚˜ì—´ë˜ì–´ ìˆìœ¼ë©´ ì§ˆë¬¸í•œ ë‚ ì§œì™€ ì¼ì¹˜í•˜ëŠ” **ëª¨ë“  ì´ë²¤íŠ¸**ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ì˜ˆ: "11ì›” 4ì¼" ì§ˆë¬¸ ì‹œ ë¬¸ì„œì—ì„œ "11ì›” 4ì¼", "4ì¼", "11/4" ë¡œ í‘œì‹œëœ ëª¨ë“  í•­ëª© ì¶”ì¶œ
- **ì œí’ˆëª… ê²€ìƒ‰**: ì‚¬ìš©ìê°€ ì œí’ˆ ì´ë¦„ì„ ë¬¼ìœ¼ë©´, ë¬¸ì„œ ë‚´ìš©ì—ì„œ ìœ ì‚¬í•œ ì´ë¦„ì„ ì°¾ìœ¼ì„¸ìš”
  * "Hê±´ê°•í”ŒëŸ¬ìŠ¤" ê²€ìƒ‰ ì‹œ â†’ "(ì¼ë°˜/ê°„í¸) Hê±´ê°•í”ŒëŸ¬ìŠ¤", "Hê±´ê°•í”ŒëŸ¬ìŠ¤ìƒí’ˆ", "ê°„í¸ Hê±´ê°•í”ŒëŸ¬ìŠ¤" ë“± ëª¨ë‘ ë§¤ì¹­
  * ì •í™•í•œ ì œí’ˆëª…ì´ ì—†ìœ¼ë©´ "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì§€ ë§ê³ , ë¬¸ì„œì—ì„œ ìœ ì‚¬í•œ ì œí’ˆì„ ì°¾ì•„ ì•ˆë‚´í•˜ì„¸ìš”

1. ì™„ì „ì„±ì´ ìµœìš°ì„ : ê²€ìƒ‰ëœ ëª¨ë“  ê´€ë ¨ í•­ëª©ì„ ë¹ ì§ì—†ì´ ë‚˜ì—´í•˜ì„¸ìš”. ì ˆëŒ€ ì¶•ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”.
2. í•­ëª© ê°œìˆ˜ ëª…ì‹œ: ì‘ë‹µ ì‹œì‘ ì‹œ ì´ ëª‡ ê°œ í•­ëª©ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: "ì´ 5ê°œ í–‰ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤")
3. ë¬¸ì„œ ë‚´ ê²€ìƒ‰: í•˜ë‚˜ì˜ ë¬¸ì„œì— ì—¬ëŸ¬ ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ ë‚ ì§œë¡œ í•„í„°ë§í•˜ì—¬ ëª¨ë‘ ì¶”ì¶œí•˜ì„¸ìš”.
4. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì†”ì§í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.

{formatting_instructions}

ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ:
ì•ˆë…•í•˜ì„¸ìš”. HO&F ì§€ì‚¬ AIì…ë‹ˆë‹¤.

11ì›” 4ì¼ í–‰ì‚¬ëŠ” ì´ 3ê°œì…ë‹ˆë‹¤.

1. IMë¼ì´í”„ ìœ„ì´‰ë§í¬ ë°œì†¡
   ê¸°ê°„: 2025ë…„ 11ì›” 4ì¼ ~ 11ì›” 6ì¼
   ìœ í˜•: ëª¨ì§‘ í–‰ì‚¬

2. KBë¼ì´í”„ ì±„ìš© í”„ë¡œëª¨ì…˜
   ê¸°ê°„: 2025ë…„ 11ì›” 4ì¼ ~ 11ì›” 30ì¼
   ì§€ì›ê¸ˆ: 500,000ì›

3. ì‹ ì… FC êµìœ¡ ì˜¤ë¦¬ì—”í…Œì´ì…˜
   ë‚ ì§œ: 2025ë…„ 11ì›” 4ì¼
   ì‹œê°„: ì˜¤ì „ 10ì‹œ
   ì¥ì†Œ: ë³¸ì‚¬ 5ì¸µ

ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”:
"""

    elif question_type == 'single':
        # USER WANTS JUST ONE ITEM
        prompt = f"""ë‹¹ì‹ ì€ HO&F ì§€ì‚¬ AIì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ í•˜ë‚˜ì˜ í•­ëª©ë§Œ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸:
{user_query}

ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ (ìµœëŒ€ 10ê°œ ë¬¸ì„œ):
{context}

**í•µì‹¬ ì§€ì¹¨:**
1. ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í•˜ë‚˜ì˜ í•­ëª©ë§Œ ì„ íƒí•˜ì„¸ìš”.
2. ì„ íƒ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”.
3. ë‹¤ë¥¸ ì˜µì…˜ì´ ìˆë‹¤ë©´ ê°„ëµíˆ ì–¸ê¸‰í•˜ì„¸ìš”.

{formatting_instructions}

ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”:
"""

    else:  # explanation or default
        # USER WANTS EXPLANATION OR GENERAL ANSWER
        prompt = f"""ë‹¹ì‹ ì€ HO&F ì§€ì‚¬ AIì…ë‹ˆë‹¤. ì•„ë˜ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸:
{user_query}

ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ (ìµœëŒ€ 10ê°œ ë¬¸ì„œ):
{context}

**í•µì‹¬ ì§€ì¹¨:**
1. ì •í™•ì„±: ê²€ìƒ‰ëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ê´€ë ¨ì„±: ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì„ íƒí•˜ì„¸ìš”.
3. êµ¬ì¡°í™”: ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
4. ì¹œì ˆí•¨: ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.

{formatting_instructions}

{commission_instructions}

ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”:
"""

    response = genai_client.models.generate_content(
        model='gemini-flash-latest',  # Use Gemini Flash for speed
        contents=prompt
    )
    return response.text


def rag_answer(user_query: str, top_k: int = 10) -> str:
    """
    Complete RAG pipeline - returns just the answer string for API use.

    Args:
        user_query: User's question
        top_k: Number of documents to retrieve (default: 10)

    Returns:
        str: Final answer from Gemini 2.5 Pro
    """
    try:
        print(f"\nğŸ” RAG Query: {user_query}")

        # Step 1: Load metadata and enhance query
        print("ğŸ”„ Step 1: Gemini Flashë¡œ ì¿¼ë¦¬ ìµœì í™” ì¤‘...")
        metadata_key = load_metadata_key()
        gemini_flash_output = enhance_query_with_gemini_flash(user_query, metadata_key)

        print(f"   âœ… ìµœì í™”ëœ ì¿¼ë¦¬: {gemini_flash_output['enhanced_query']}")
        if gemini_flash_output['filters']:
            print(f"   ğŸ¯ í•„í„°: {json.dumps(gemini_flash_output['filters'], ensure_ascii=False)}")

        # Step 2: Retrieve from Pinecone (retrieve top 10 for AI to choose from)
        print(f"ğŸ” Step 2: Pineconeì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘ (namespace: {NAMESPACE}, top {top_k})...")
        results = retrieve_from_pinecone(
            gemini_flash_output['enhanced_query'],
            gemini_flash_output['filters'],
            top_k=top_k
        )

        print(f"   âœ… {len(results.matches)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")

        # Fallback: If no results with filters, retry without filters (pure semantic search)
        if len(results.matches) == 0 and gemini_flash_output['filters'] is not None:
            print(f"   âš ï¸ í•„í„° ì ìš© ê²°ê³¼ 0ê°œ - í•„í„° ì—†ì´ ì¬ê²€ìƒ‰ ì¤‘...")
            results = retrieve_from_pinecone(
                gemini_flash_output['enhanced_query'],
                filters=None,  # No filters, pure semantic search
                top_k=top_k
            )
            print(f"   âœ… ì¬ê²€ìƒ‰ ì™„ë£Œ: {len(results.matches)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (ìˆœìˆ˜ ì‹œë§¨í‹± ê²€ìƒ‰)")

        # Check relevance scores - if all results have low scores, ask for more specific query
        RELEVANCE_THRESHOLD = 0.3  # Threshold for considering results relevant
        if results.matches:
            max_score = max(match.score for match in results.matches)
            print(f"   ğŸ“Š ìµœê³  ê´€ë ¨ë„ ì ìˆ˜: {max_score:.3f}")

            # Check for generic greetings or inappropriate queries
            low_quality_keywords = ['hey', 'hi', 'hello', 'ì•ˆë…•', 'í•˜ì´', 'ìš•', 'ì”¨ë°œ', 'ê°œìƒˆ', 'ë³‘ì‹ ', 'fuck', 'shit']
            is_low_quality = any(keyword in user_query.lower() for keyword in low_quality_keywords)

            if max_score < RELEVANCE_THRESHOLD or (is_low_quality and max_score < 0.5):
                print(f"   âš ï¸ ë‚®ì€ ê´€ë ¨ë„ ê°ì§€ ë˜ëŠ” ë¶€ì ì ˆí•œ ì¿¼ë¦¬")
                import datetime
                from datetime import datetime as dt

                now = dt.now()
                # Format time in Korean style
                weekdays = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
                weekday = weekdays[now.weekday()]

                if now.hour < 12:
                    ampm = "ì˜¤ì „"
                    hour_12 = now.hour if now.hour != 0 else 12
                else:
                    ampm = "ì˜¤í›„"
                    hour_12 = now.hour if now.hour <= 12 else now.hour - 12

                time_str = f"{now.year}ë…„ {now.month}ì›” {now.day}ì¼ ({weekday}) {ampm} {hour_12}ì‹œ {now.minute}ë¶„"

                return f"""ì•ˆë…•í•˜ì„¸ìš”. HO&F ì§€ì‚¬ AIì…ë‹ˆë‹¤.

í˜„ì¬ ì‹œê°: {time_str}

ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.

êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆì‹œ:
- 11ì›” ì›Œí¬ìƒµ ì¼ì • ì•Œë ¤ì¤˜
- ì‚¼ì„±í™”ì¬ í”„ë¡œëª¨ì…˜ ì •ë³´
- ì‹ ì… FC êµìœ¡ ì¼ì •
- í™˜ìˆ˜ ê·œì • ì•Œë ¤ì¤˜

ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"""

        # Format context
        context = format_context_for_gemini(results)

        # Step 3: Generate answer with Gemini 2.5 Pro
        print("ğŸ’¬ Step 3: Gemini 2.5 Proë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
        answer = generate_answer_with_gemini_pro(user_query, context)

        print(f"   âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(answer)}ì)")

        # Step 4: Attach relevant PDFs
        print("ğŸ“ Step 4: ê´€ë ¨ PDF ì²¨ë¶€ ì¤‘...")
        relevant_pdfs = get_relevant_pdfs(user_query, results)
        if relevant_pdfs:
            pdf_attachments = format_pdf_attachments(relevant_pdfs)
            answer += pdf_attachments
            print(f"   âœ… {len(relevant_pdfs)}ê°œ PDF ì²¨ë¶€ ì™„ë£Œ\n")
        else:
            print(f"   â„¹ï¸  ì²¨ë¶€í•  PDF ì—†ìŒ\n")

        return answer

    except Exception as e:
        print(f"âŒ RAG ì˜¤ë¥˜: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# For backward compatibility with existing code
def getTextFromGPT_RAG(prompt: str) -> str:
    """
    Wrapper function to replace the old getTextFromGPT function.
    Uses the new RAG pipeline with top 10 results.
    """
    return rag_answer(prompt, top_k=10)
